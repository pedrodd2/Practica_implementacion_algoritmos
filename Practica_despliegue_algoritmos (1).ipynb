{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cj8bcxFX85UO"
      },
      "source": [
        "# Práctica Final: detección de mensajes troll\n",
        "\n",
        "## Ejemplo de solución.\n",
        "\n",
        "**Disclaimer.** Existen distintas formas de resolver el ejercicio y todas se considerarán correctas siempre que el modelo se entrene y despliegue acorde a lo esperado.\n",
        "\n",
        "En los recientes años, Twitch no sólo ha surgido como un pionero en la comunicación moderna, sino también como una un sitio donde las nuevas generaciones pasan el tiempo ver a otros jovenes haciendo cosas por internet. Esta plataforma, rica en interactividad, permite a los usuarios compartir sus pensamientos en tiempo real y después de las transmisiones. Sin embargo, entre las voces genuinas, algunas buscan dañar con comentarios ofensivos.\n",
        "\n",
        "Pero hoy, nos embarcamos en una misión inspiradora: diseñar una Inteligencia Artificial que pueda identificar y neutralizar estos mensajes tóxicos. A lo largo de esta práctica, no solo entrenaremos un modelo de Deep Learning avanzado, sino que también lo implementaremos para inferencias en batch, estableciendo un nuevo estándar en la industria."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hgE9XB8ACVmQ"
      },
      "source": [
        "# Configuración de nuestro proyecto en GCP\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zAorVw7RCT9C",
        "outputId": "9630392c-bb93-472f-9bab-b7af69d4fe07",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updated property [core/project].\n"
          ]
        }
      ],
      "source": [
        "PROJECT_ID = \"august-button-377008\" #@param {type:\"string\"}\n",
        "! gcloud config set project $PROJECT_ID"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "hbAl8pSkCXCm",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "# If you are running this notebook in Colab, run this cell and follow the\n",
        "# instructions to authenticate your GCP account. This provides access to your\n",
        "# Cloud Storage bucket and lets you submit training jobs and prediction\n",
        "# requests.\n",
        "\n",
        "if 'google.colab' in sys.modules:\n",
        "  from google.colab import auth as google_auth\n",
        "  google_auth.authenticate_user()\n",
        "\n",
        "# If you are running this notebook locally, replace the string below with the\n",
        "# path to your service account key and run this cell to authenticate your GCP\n",
        "# account.\n",
        "else:\n",
        "  %env GOOGLE_APPLICATION_CREDENTIALS ''\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p5Xoej7vH0X-"
      },
      "source": [
        "Creamos el bucket mediante la consola y una vez creado lo indicamos en la variable:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "n6Zpx14FCfXy",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "BUCKET_NAME = \"practicadesplieguealgoritmos2\" #@param {type:\"string\"}\n",
        "REGION = \"europe-west1\" #@param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "lE4V-Xt0ChSb",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "! gsutil ls -al gs://$BUCKET_NAME"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zfbep8bLWBau",
        "outputId": "257c7a61-6838-45f5-944d-75e17ca4bd47",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating gs://practicadesplieguealgoritmos2/...\n"
          ]
        }
      ],
      "source": [
        "#! gsutil mb -l $REGION gs://$BUCKET_NAME"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3D8Qw14y_nRa"
      },
      "source": [
        "# Entrenamiento e inferencia en Batch\n",
        "\n",
        "## Preparación\n",
        "\n",
        "Para esta primera parte se va a utilizar [Tweets Dataset for Detection of Cyber-Trolls](https://www.kaggle.com/dataturks/dataset-for-detection-of-cybertrolls). El objetivo es desarrollar un clasificador binario para detectar si el mensaje recibido es troll (1) o no (0). **Las métricas obtenidas del entrenamiento y la inferencia no se tendrán en cuenta para la evaluación de la práctica, la importancia está en la arquitectura de la solución**, es decir, lo importante no es que nuestro modelo detecte correctamente los tweets de trolls si no que funcione y sea capaz de hacer inferencias.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qM_dh-471gIq"
      },
      "source": [
        "A continuación, se van a subir los datos de entrenamiento al bucket del proyecto que se haya creado. **Importante:** crea el bucket en una única región. Os dejo disponibilizado el dataset en un bucket de acceso público:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sqp4L_nmUjAc",
        "outputId": "f6c49f91-6bb7-4c62-a40c-2bc4e27be811",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gdown in /usr/local/lib/python3.10/dist-packages (4.6.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from gdown) (3.12.2)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.10/dist-packages (from gdown) (2.31.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from gdown) (1.16.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from gdown) (4.66.1)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown) (4.11.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown) (2.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2023.7.22)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (1.7.1)\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1dTaKofC9ZcMWa5cVtGLDFkEnbc4hiPJr\n",
            "To: /content/dataset-cybertrolls.json\n",
            "100% 2.76M/2.76M [00:00<00:00, 225MB/s]\n"
          ]
        }
      ],
      "source": [
        "%pip install gdown\n",
        "! gdown \"1dTaKofC9ZcMWa5cVtGLDFkEnbc4hiPJr\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vCQQ9j2I11tg"
      },
      "source": [
        "Ahora se crea el directorio dónde vas a desarrollar esta primera parte de la práctica."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PsblBlJ6RrGm",
        "outputId": "2426e5ed-a46c-458c-97be-8da76d65d6ee",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘/content/batch’: File exists\n"
          ]
        }
      ],
      "source": [
        "%mkdir /content/batch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nyK51quU1_oi"
      },
      "source": [
        "Se establece el directorio de trabajo que hemos creado."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s7ybSaotRwkP",
        "outputId": "06b925c6-942a-49cf-d593-9edfec911868",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/batch\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "# Set the working directory to the sample code directory\n",
        "%cd /content/batch\n",
        "\n",
        "WORK_DIR = os.getcwd()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4NSd4bAo2aj_"
      },
      "source": [
        "Ahora se descargarán los datos en el workspace de Colab para trabajar en local."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qOmUJSg1JCgg",
        "outputId": "5b08cb65-a220-4d0d-e7f2-c51baed087dd",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1dTaKofC9ZcMWa5cVtGLDFkEnbc4hiPJr\n",
            "To: /content/batch/dataset-cybertrolls.json\n",
            "\r  0% 0.00/2.76M [00:00<?, ?B/s]\r100% 2.76M/2.76M [00:00<00:00, 201MB/s]\n"
          ]
        }
      ],
      "source": [
        "! gdown \"1dTaKofC9ZcMWa5cVtGLDFkEnbc4hiPJr\"\n",
        "! mv dataset-cybertrolls.json dataset.json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eRoW6zyg2kEz"
      },
      "source": [
        "Se establecen las dependencias que se usarán en la práctica. Se pueden añadir y quitar las dependencias que no se usen o viceversa."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9s0-8_JK_z2D",
        "outputId": "e0a0a5ea-78d2-4326-d3f6-5095a9c20b40",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting requirements.txt\n"
          ]
        }
      ],
      "source": [
        "%%writefile requirements.txt\n",
        "\n",
        "apache-beam[gcp]\n",
        "tensorflow\n",
        "gensim\n",
        "fsspec\n",
        "gcsfs\n",
        "numpy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yIl8lrPp2x2j"
      },
      "source": [
        "Instalamos las dependencias. **No olvides reiniciar el entorno al instalar y establecer las variables y credenciales de GCP al arrancar.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "74dHb1iQ0UNN",
        "outputId": "098c51dc-b1fc-4feb-dd4e-ac13c0bea3c6",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: apache-beam[gcp] in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 2)) (2.50.0)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 3)) (2.13.0)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 4)) (4.3.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 5)) (2023.6.0)\n",
            "Requirement already satisfied: gcsfs in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 6)) (2023.6.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 7)) (1.23.5)\n",
            "Requirement already satisfied: crcmod<2.0,>=1.7 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]->-r requirements.txt (line 2)) (1.7)\n",
            "Requirement already satisfied: orjson<4.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]->-r requirements.txt (line 2)) (3.9.7)\n",
            "Requirement already satisfied: dill<0.3.2,>=0.3.1.1 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]->-r requirements.txt (line 2)) (0.3.1.1)\n",
            "Requirement already satisfied: cloudpickle~=2.2.1 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]->-r requirements.txt (line 2)) (2.2.1)\n",
            "Requirement already satisfied: fastavro<2,>=0.23.6 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]->-r requirements.txt (line 2)) (1.8.3)\n",
            "Requirement already satisfied: fasteners<1.0,>=0.3 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]->-r requirements.txt (line 2)) (0.18)\n",
            "Requirement already satisfied: grpcio!=1.48.0,<2,>=1.33.1 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]->-r requirements.txt (line 2)) (1.57.0)\n",
            "Requirement already satisfied: hdfs<3.0.0,>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]->-r requirements.txt (line 2)) (2.7.2)\n",
            "Requirement already satisfied: httplib2<0.23.0,>=0.8 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]->-r requirements.txt (line 2)) (0.22.0)\n",
            "Requirement already satisfied: objsize<0.7.0,>=0.6.1 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]->-r requirements.txt (line 2)) (0.6.1)\n",
            "Requirement already satisfied: pymongo<5.0.0,>=3.8.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]->-r requirements.txt (line 2)) (4.5.0)\n",
            "Requirement already satisfied: proto-plus<2,>=1.7.1 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]->-r requirements.txt (line 2)) (1.22.3)\n",
            "Requirement already satisfied: protobuf<4.24.0,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]->-r requirements.txt (line 2)) (3.20.3)\n",
            "Requirement already satisfied: pydot<2,>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]->-r requirements.txt (line 2)) (1.4.2)\n",
            "Requirement already satisfied: python-dateutil<3,>=2.8.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]->-r requirements.txt (line 2)) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2018.3 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]->-r requirements.txt (line 2)) (2023.3.post1)\n",
            "Requirement already satisfied: regex>=2020.6.8 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]->-r requirements.txt (line 2)) (2023.6.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.24.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]->-r requirements.txt (line 2)) (2.31.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]->-r requirements.txt (line 2)) (4.5.0)\n",
            "Requirement already satisfied: zstandard<1,>=0.18.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]->-r requirements.txt (line 2)) (0.21.0)\n",
            "Requirement already satisfied: pyarrow<12.0.0,>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]->-r requirements.txt (line 2)) (9.0.0)\n",
            "Requirement already satisfied: cachetools<6,>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]->-r requirements.txt (line 2)) (5.3.1)\n",
            "Requirement already satisfied: google-api-core<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]->-r requirements.txt (line 2)) (2.11.1)\n",
            "Requirement already satisfied: google-apitools<0.5.32,>=0.5.31 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]->-r requirements.txt (line 2)) (0.5.31)\n",
            "Requirement already satisfied: google-auth<3,>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]->-r requirements.txt (line 2)) (2.17.3)\n",
            "Requirement already satisfied: google-auth-httplib2<0.2.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]->-r requirements.txt (line 2)) (0.1.0)\n",
            "Requirement already satisfied: google-cloud-datastore<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]->-r requirements.txt (line 2)) (2.15.2)\n",
            "Requirement already satisfied: google-cloud-pubsub<3,>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]->-r requirements.txt (line 2)) (2.18.4)\n",
            "Requirement already satisfied: google-cloud-pubsublite<2,>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]->-r requirements.txt (line 2)) (1.8.3)\n",
            "Requirement already satisfied: google-cloud-bigquery<4,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]->-r requirements.txt (line 2)) (3.10.0)\n",
            "Requirement already satisfied: google-cloud-bigquery-storage<3,>=2.6.3 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]->-r requirements.txt (line 2)) (2.22.0)\n",
            "Requirement already satisfied: google-cloud-core<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]->-r requirements.txt (line 2)) (2.3.3)\n",
            "Requirement already satisfied: google-cloud-bigtable<3,>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]->-r requirements.txt (line 2)) (2.21.0)\n",
            "Requirement already satisfied: google-cloud-spanner<4,>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]->-r requirements.txt (line 2)) (3.40.1)\n",
            "Requirement already satisfied: google-cloud-dlp<4,>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]->-r requirements.txt (line 2)) (3.12.3)\n",
            "Requirement already satisfied: google-cloud-language<3,>=2.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]->-r requirements.txt (line 2)) (2.9.1)\n",
            "Requirement already satisfied: google-cloud-videointelligence<3,>=2.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]->-r requirements.txt (line 2)) (2.11.3)\n",
            "Requirement already satisfied: google-cloud-vision<4,>=2 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]->-r requirements.txt (line 2)) (3.4.4)\n",
            "Requirement already satisfied: google-cloud-recommendations-ai<0.11.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]->-r requirements.txt (line 2)) (0.10.4)\n",
            "Requirement already satisfied: google-cloud-aiplatform<2.0,>=1.26.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]->-r requirements.txt (line 2)) (1.32.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r requirements.txt (line 3)) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r requirements.txt (line 3)) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.1.21 in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r requirements.txt (line 3)) (23.5.26)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r requirements.txt (line 3)) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r requirements.txt (line 3)) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r requirements.txt (line 3)) (3.9.0)\n",
            "Requirement already satisfied: keras<2.14,>=2.13.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r requirements.txt (line 3)) (2.13.1)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r requirements.txt (line 3)) (16.0.6)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r requirements.txt (line 3)) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r requirements.txt (line 3)) (23.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r requirements.txt (line 3)) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r requirements.txt (line 3)) (1.16.0)\n",
            "Requirement already satisfied: tensorboard<2.14,>=2.13 in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r requirements.txt (line 3)) (2.13.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.14,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r requirements.txt (line 3)) (2.13.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r requirements.txt (line 3)) (2.3.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r requirements.txt (line 3)) (1.15.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r requirements.txt (line 3)) (0.33.0)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim->-r requirements.txt (line 4)) (1.11.2)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim->-r requirements.txt (line 4)) (6.4.0)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.10/dist-packages (from gcsfs->-r requirements.txt (line 6)) (3.8.5)\n",
            "Requirement already satisfied: decorator>4.1.2 in /usr/local/lib/python3.10/dist-packages (from gcsfs->-r requirements.txt (line 6)) (4.4.2)\n",
            "Requirement already satisfied: google-auth-oauthlib in /usr/local/lib/python3.10/dist-packages (from gcsfs->-r requirements.txt (line 6)) (1.0.0)\n",
            "Requirement already satisfied: google-cloud-storage in /usr/local/lib/python3.10/dist-packages (from gcsfs->-r requirements.txt (line 6)) (2.8.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs->-r requirements.txt (line 6)) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs->-r requirements.txt (line 6)) (3.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs->-r requirements.txt (line 6)) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs->-r requirements.txt (line 6)) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs->-r requirements.txt (line 6)) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs->-r requirements.txt (line 6)) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs->-r requirements.txt (line 6)) (1.3.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow->-r requirements.txt (line 3)) (0.41.2)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core<3,>=2.0.0->apache-beam[gcp]->-r requirements.txt (line 2)) (1.60.0)\n",
            "Requirement already satisfied: oauth2client>=1.4.12 in /usr/local/lib/python3.10/dist-packages (from google-apitools<0.5.32,>=0.5.31->apache-beam[gcp]->-r requirements.txt (line 2)) (4.1.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.18.0->apache-beam[gcp]->-r requirements.txt (line 2)) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.18.0->apache-beam[gcp]->-r requirements.txt (line 2)) (4.9)\n",
            "Requirement already satisfied: google-cloud-resource-manager<3.0.0dev,>=1.3.3 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform<2.0,>=1.26.0->apache-beam[gcp]->-r requirements.txt (line 2)) (1.10.3)\n",
            "Requirement already satisfied: shapely<2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform<2.0,>=1.26.0->apache-beam[gcp]->-r requirements.txt (line 2)) (1.8.5.post1)\n",
            "Requirement already satisfied: google-resumable-media<3.0dev,>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-bigquery<4,>=2.0.0->apache-beam[gcp]->-r requirements.txt (line 2)) (2.6.0)\n",
            "Requirement already satisfied: grpc-google-iam-v1<1.0.0dev,>=0.12.4 in /usr/local/lib/python3.10/dist-packages (from google-cloud-bigtable<3,>=2.19.0->apache-beam[gcp]->-r requirements.txt (line 2)) (0.12.6)\n",
            "Requirement already satisfied: grpcio-status>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-cloud-pubsub<3,>=2.1.0->apache-beam[gcp]->-r requirements.txt (line 2)) (1.48.2)\n",
            "Requirement already satisfied: overrides<7.0.0,>=6.0.1 in /usr/local/lib/python3.10/dist-packages (from google-cloud-pubsublite<2,>=1.2.0->apache-beam[gcp]->-r requirements.txt (line 2)) (6.5.0)\n",
            "Requirement already satisfied: sqlparse>=0.4.4 in /usr/local/lib/python3.10/dist-packages (from google-cloud-spanner<4,>=3.0.0->apache-beam[gcp]->-r requirements.txt (line 2)) (0.4.4)\n",
            "Requirement already satisfied: docopt in /usr/local/lib/python3.10/dist-packages (from hdfs<3.0.0,>=2.1.0->apache-beam[gcp]->-r requirements.txt (line 2)) (0.6.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.10/dist-packages (from httplib2<0.23.0,>=0.8->apache-beam[gcp]->-r requirements.txt (line 2)) (3.1.1)\n",
            "Requirement already satisfied: dnspython<3.0.0,>=1.16.0 in /usr/local/lib/python3.10/dist-packages (from pymongo<5.0.0,>=3.8.0->apache-beam[gcp]->-r requirements.txt (line 2)) (2.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.24.0->apache-beam[gcp]->-r requirements.txt (line 2)) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.24.0->apache-beam[gcp]->-r requirements.txt (line 2)) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.24.0->apache-beam[gcp]->-r requirements.txt (line 2)) (2023.7.22)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow->-r requirements.txt (line 3)) (3.4.4)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow->-r requirements.txt (line 3)) (0.7.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow->-r requirements.txt (line 3)) (2.3.7)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib->gcsfs->-r requirements.txt (line 6)) (1.3.1)\n",
            "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /usr/local/lib/python3.10/dist-packages (from google-resumable-media<3.0dev,>=0.6.0->google-cloud-bigquery<4,>=2.0.0->apache-beam[gcp]->-r requirements.txt (line 2)) (1.5.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.10/dist-packages (from oauth2client>=1.4.12->google-apitools<0.5.32,>=0.5.31->apache-beam[gcp]->-r requirements.txt (line 2)) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib->gcsfs->-r requirements.txt (line 6)) (3.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.14,>=2.13->tensorflow->-r requirements.txt (line 3)) (2.1.3)\n"
          ]
        }
      ],
      "source": [
        "! pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k0DDu27S3CJv"
      },
      "source": [
        "## Primer ejercicio\n",
        "\n",
        "Desarrollar un pipeline de preprocesamiento utilizando Apache Beam para generar datos de train, eval y test para los datos proporcionados anteriormente. Requisitos:\n",
        "\n",
        "- Proporcionar dos modos de ejecución: `train` y `test`\n",
        "- Soportar ejecuciones en local con `DirectRunner` y ejecuciones en Dataflow usando `DataFlowRunner`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1blctlxs_dPO",
        "outputId": "adf0576a-c733-4d7e-9e90-1cd86e620129",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing preprocess.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile preprocess.py\n",
        "\n",
        "# Licensed to the Apache Software Foundation (ASF) under one or more\n",
        "# contributor license agreements.  See the NOTICE file distributed with\n",
        "# this work for additional information regarding copyright ownership.\n",
        "# The ASF licenses this file to You under the Apache License, Version 2.0\n",
        "# (the \"License\"); you may not use this file except in compliance with\n",
        "# the License.  You may obtain a copy of the License at\n",
        "#\n",
        "#    http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "#\n",
        "\n",
        "\"\"\"A word-counting workflow.\"\"\"\n",
        "\n",
        "# pytype: skip-file\n",
        "\n",
        "from __future__ import absolute_import\n",
        "\n",
        "\n",
        "import argparse\n",
        "import logging\n",
        "import re\n",
        "import os\n",
        "import csv\n",
        "import random\n",
        "import json\n",
        "from past.builtins import unicode\n",
        "\n",
        "import apache_beam as beam\n",
        "from apache_beam.io import ReadFromText #fichero de texto a colección\n",
        "from apache_beam.io import WriteToText #colección a fichero de texto\n",
        "from apache_beam.coders.coders import Coder #gestión de codificación de archivos\n",
        "from apache_beam.options.pipeline_options import PipelineOptions\n",
        "from apache_beam.options.pipeline_options import SetupOptions, DirectOptions\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import SnowballStemmer\n",
        "\n",
        "nltk.download(\"stopwords\")\n",
        "\n",
        "# CLEANING\n",
        "STOP_WORDS = stopwords.words(\"english\") #stopwords en inglés\n",
        "STEMMER = SnowballStemmer(\"english\") #stemmer en inglés\n",
        "TEXT_CLEANING_RE = \"@\\S+|https?:\\S+|http?:\\S|[^A-Za-z0-9]+\" #Expresión regular para limpiar urls\n",
        "\n",
        "\n",
        "class CustomCoder(Coder):\n",
        "    \"\"\"Custom coder utilizado para ller y escribir strings. Realiza una serie de tranformaciones entre codificaciones\"\"\"\n",
        "\n",
        "    def __init__(self, encoding: str):\n",
        "        # latin-1\n",
        "        # iso-8859-1\n",
        "        self.enconding = encoding\n",
        "\n",
        "    def encode(self, value):\n",
        "        return value.encode(self.enconding)\n",
        "\n",
        "    def decode(self, value):\n",
        "        return value.decode(self.enconding)\n",
        "\n",
        "    def is_deterministic(self):\n",
        "        return True\n",
        "\n",
        "class ExtractColumnsDoFn(beam.DoFn):    #Extrae información de las columnas\n",
        "    def process(self, element):\n",
        "        # space removal\n",
        "        element_json = json.loads(element)\n",
        "        # Extract relevant information\n",
        "\n",
        "        text = element_json[\"content\"]\n",
        "        sentiment = element_json[\"annotation\"][\"label\"][0]\n",
        "        # Emit the extracted information as a tuple\n",
        "        yield (text, sentiment)\n",
        "\n",
        "\n",
        "class PreprocessColumnsTrainFn(beam.DoFn): #realizado todo el preprocesamiento propio de NLP\n",
        "    def process_sentiment(self, sentiment):  #preprocesa la parte de sentimiento\n",
        "        sentiment = int(sentiment)\n",
        "        if sentiment == 1:\n",
        "            return \"POSITIVE\"\n",
        "        elif sentiment == 0:\n",
        "            return \"NEGATIVE\"\n",
        "        else:\n",
        "            return \"NEUTRAL\"\n",
        "\n",
        "    def process_text(self, text): #preprocesa la parte de texto\n",
        "        # Elimina link,user y caracteres especiales\n",
        "        stem = False\n",
        "        text = re.sub(TEXT_CLEANING_RE, \" \", str(text).lower()).strip() #limpiar URL\n",
        "        tokens = []\n",
        "        for token in text.split():\n",
        "            if token not in STOP_WORDS:\n",
        "                if stem:\n",
        "                    tokens.append(STEMMER.stem(token)) #stemmiza\n",
        "                else:\n",
        "                    tokens.append(token)\n",
        "        return \" \".join(tokens) #devuelve el resultado\n",
        "\n",
        "    def process(self, element):\n",
        "        processed_text = self.process_text(element[0]) #procesa texto\n",
        "        processed_sentiment = self.process_sentiment(element[1]) #procesa sentimiento\n",
        "        yield f\"{processed_text}, {processed_sentiment}\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def run(argv=None, save_main_session=True):\n",
        "\n",
        "  \"\"\"Main entry point; defines and runs the wordcount pipeline.\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "  parser = argparse.ArgumentParser() #instanciamos el parseador de argumentos\n",
        "\n",
        "  parser.add_argument( #añado argumento y lo guardo en la variable work_dir. Directorio de trabajo\n",
        "        \"--work-dir\", dest=\"work_dir\", required=True, help=\"Working directory\",\n",
        "  )\n",
        "\n",
        "  parser.add_argument( # añado argumento. Fichero de entrada\n",
        "        \"--input\", dest=\"input\", required=True, help=\"Input dataset in work dir\",\n",
        "    )\n",
        "  parser.add_argument( # añado argumento. Fichero de salida\n",
        "        \"--output\",\n",
        "        dest=\"output\",\n",
        "        required=True,\n",
        "        help=\"Output path to store transformed data in work dir\",\n",
        "    )\n",
        "  parser.add_argument( # añado argumento. Indica si estamos entrenando o evaluando\n",
        "        \"--mode\",\n",
        "        dest=\"mode\",\n",
        "        required=True,\n",
        "        choices=[\"train\", \"test\"],\n",
        "        help=\"Type of output to store transformed data\",\n",
        "    )\n",
        "\n",
        "  known_args, pipeline_args = parser.parse_known_args(argv) #recoge los argumentos enviados por consola y los almacena\n",
        "\n",
        "  # Añadimos configuración de la pipeline\n",
        "  pipeline_options = PipelineOptions(pipeline_args)\n",
        "  pipeline_options.view_as(SetupOptions).save_main_session = save_main_session #permite usar dependencias externas como nltk\n",
        "  pipeline_options.view_as(DirectOptions).direct_num_workers = 0 #número de workers. 0 para que coja el número máximo que tiene la máquina\n",
        "\n",
        "  # Hasta aquí hemos añadido la configuración, a partir de ahora comenzamos a construir la pipeline:\n",
        "  with beam.Pipeline(options=pipeline_options) as p:\n",
        "    # Read the text file[pattern] into a PCollection.\n",
        "    raw_data = p | \"ReadTwitterData\" >> ReadFromText(\n",
        "            known_args.input, coder=CustomCoder(\"latin-1\")) #leemos el texto y tratamos la codificación. Hemos dado un nombre a la transformación\n",
        "\n",
        "    if known_args.mode == \"train\":\n",
        "\n",
        "      transformed_data = ( #construimos datos transformados como la aplicación al raw_data de nuestras dos transformacione\n",
        "                raw_data\n",
        "                | \"ExtractColumns\" >> beam.ParDo(ExtractColumnsDoFn()) #Extreamos información de columna\n",
        "                | \"Preprocess\" >> beam.ParDo(PreprocessColumnsTrainFn()) #Preprocesamos la información\n",
        "            )\n",
        "      #Vamos a construir un conjunto de validación\n",
        "      eval_percent = 20\n",
        "      assert 0 < eval_percent < 100, \"eval_percent must in the range (0-100)\"\n",
        "      train_dataset, eval_dataset = (\n",
        "                transformed_data\n",
        "                | \"Split dataset\"\n",
        "                >> beam.Partition( #particionamos nuestros datos cumpliendo\n",
        "                    lambda elem, _: int(random.uniform(0, 100) < eval_percent), 2\n",
        "                ) # el 2 es el número de particiones\n",
        "            )\n",
        "\n",
        "      train_dataset | \"TrainWriteToCSV\" >> WriteToText( # escribimos el conjunto de train en un csv\n",
        "                os.path.join(known_args.output, \"train\", \"part\") #part genera las distintas particiones para cada fichero. Es el prefijo del fichero.\n",
        "            )\n",
        "      eval_dataset | \"EvalWriteToCSV\" >> WriteToText( # escribimos el conjunto de eval en un csv\n",
        "                os.path.join(known_args.output, \"eval\", \"part\") #part genera las distintas particiones para cada fichero. Es el prefijo del fichero.\n",
        "            )\n",
        "\n",
        "    else:\n",
        "      transformed_data = (\n",
        "                raw_data\n",
        "                | \"ExtractColumns\" >> beam.ParDo(ExtractColumnsDoFn()) #De nuevo extraemos la información de columnas\n",
        "                | \"Preprocess\" >> beam.Map(lambda x: f'\"{x[0]}\"') # En este caso solo nos quedamos con la columna de texto sin preprocesar.\n",
        "            )\n",
        "\n",
        "      transformed_data | \"TestWriteToCSV\" >> WriteToText(\n",
        "                os.path.join(known_args.output, \"test\", \"part\") #escribimos de nuevo a un fichero de texto.\n",
        "            )\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    logging.getLogger().setLevel(logging.INFO)\n",
        "    run()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qawuKC0k4B0e"
      },
      "source": [
        "Se proporciona un fichero `setup.py` necesario para ejecutar en DataFlow. Modificar la variable `REQUIRED_PACKAGES` con las dependencias que se hayan usado en el `requirements.txt`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n1MQvWsk_mVX",
        "outputId": "db8e4442-266e-4d43-f909-cb64d8c16411",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing setup.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile setup.py\n",
        "\n",
        "import setuptools\n",
        "\n",
        "REQUIRED_PACKAGES = [\n",
        "  \"apache-beam[gcp]\",\n",
        "  \"tensorflow\",\n",
        "  \"gensim\",\n",
        "  \"fsspec\",\n",
        "  \"gcsfs\",\n",
        "  \"numpy\"\n",
        "]\n",
        "\n",
        "setuptools.setup(\n",
        "    name=\"twitchstreaming\",\n",
        "    version=\"0.0.1\",\n",
        "    install_requires=REQUIRED_PACKAGES,\n",
        "    packages=setuptools.find_packages(),\n",
        "    include_package_data=True,\n",
        "    description=\"Troll detection\",\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hxYFYMy-o55h"
      },
      "source": [
        "Me creo una copia por si hubiera algún error en el procesamiento (buena práctica):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KlBCbvSQpBUK",
        "outputId": "71a5c6d9-dd8a-4656-93fd-4e008078d36c",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1dTaKofC9ZcMWa5cVtGLDFkEnbc4hiPJr\n",
            "To: /content/batch/dataset-cybertrolls.json\n",
            "100% 2.76M/2.76M [00:00<00:00, 58.2MB/s]\n",
            "Copying file://dataset-cybertrolls.json...\n",
            "/ [1 files][  2.6 MiB/  2.6 MiB]                                                \n",
            "Operation completed over 1 objects/2.6 MiB.                                      \n"
          ]
        }
      ],
      "source": [
        "! gdown \"1dTaKofC9ZcMWa5cVtGLDFkEnbc4hiPJr\"\n",
        "! gsutil cp dataset-cybertrolls.json //$WORK_DIR/data.json\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jLDlz_fL4UJA"
      },
      "source": [
        "### Validación preprocess train en local\n",
        "Con el comando mostrado a continuación se valida la correcta generación de los datos de entrenamiento y validación en local."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vFpirg37C3bN",
        "outputId": "e3344d29-15e2-4048-b26b-282b49afa726",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "WARNING:apache_beam.runners.portability.fn_api_runner.fn_runner:If direct_num_workers is not equal to 1, direct_running_mode should be `multi_processing` or `multi_threading` instead of `in_memory` in order for it to have the desired worker parallelism effect. direct_num_workers: 2 ; running_mode: in_memory\n",
            "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function annotate_downstream_side_inputs at 0x7cc2bb0c81f0> ====================\n",
            "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function fix_side_input_pcoll_coders at 0x7cc2bb0c8310> ====================\n",
            "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function pack_combiners at 0x7cc2bb0c8820> ====================\n",
            "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function lift_combiners at 0x7cc2bb0c88b0> ====================\n",
            "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function expand_sdf at 0x7cc2bb0c8a60> ====================\n",
            "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function expand_gbk at 0x7cc2bb0c8af0> ====================\n",
            "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function sink_flattens at 0x7cc2bb0c8c10> ====================\n",
            "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function greedily_fuse at 0x7cc2bb0c8ca0> ====================\n",
            "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function read_to_impulse at 0x7cc2bb0c8d30> ====================\n",
            "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function impulse_to_input at 0x7cc2bb0c8dc0> ====================\n",
            "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function sort_stages at 0x7cc2bb0c9000> ====================\n",
            "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function add_impulse_to_dangling_transforms at 0x7cc2bb0c9120> ====================\n",
            "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function setup_timer_mapping at 0x7cc2bb0c8f70> ====================\n",
            "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function populate_data_channel_coders at 0x7cc2bb0c9090> ====================\n",
            "INFO:apache_beam.runners.worker.statecache:Creating state cache with size 104857600\n",
            "INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:Created Worker handler <apache_beam.runners.portability.fn_api_runner.worker_handlers.EmbeddedWorkerHandler object at 0x7cc2bae1d1e0> for environment ref_Environment_default_environment_1 (beam:env:embedded_python:v1, b'')\n",
            "INFO:apache_beam.runners.worker.statecache:Creating state cache with size 104857600\n",
            "INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:Created Worker handler <apache_beam.runners.portability.fn_api_runner.worker_handlers.EmbeddedWorkerHandler object at 0x7cc2bae1ed10> for environment ref_Environment_default_environment_1 (beam:env:embedded_python:v1, b'')\n",
            "INFO:apache_beam.io.filebasedsink:Starting finalize_write threads with num_shards: 2 (skipped: 0), batches: 2, num_threads: 2\n",
            "INFO:apache_beam.io.filebasedsink:Renamed 2 shards in 0.00 seconds.\n",
            "INFO:apache_beam.io.filebasedsink:Starting finalize_write threads with num_shards: 2 (skipped: 0), batches: 2, num_threads: 2\n",
            "INFO:apache_beam.io.filebasedsink:Renamed 2 shards in 0.00 seconds.\n"
          ]
        }
      ],
      "source": [
        "! python3 preprocess.py \\\n",
        "  --work-dir $WORK_DIR \\\n",
        "  --runner DirectRunner \\\n",
        "  --input $WORK_DIR/data.json \\\n",
        "  --output $WORK_DIR/transformed_data \\\n",
        "  --mode train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wkkG271a4qnA"
      },
      "source": [
        "### Validación preprocess test en local\n",
        "\n",
        "Con el comando mostrado a continuación se valida la correcta generación de los datos de test en local."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u4lkLgecLS3i",
        "outputId": "537a7a6d-d1f4-4583-d08d-69bf4dd9708c",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "WARNING:apache_beam.runners.portability.fn_api_runner.fn_runner:If direct_num_workers is not equal to 1, direct_running_mode should be `multi_processing` or `multi_threading` instead of `in_memory` in order for it to have the desired worker parallelism effect. direct_num_workers: 2 ; running_mode: in_memory\n",
            "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function annotate_downstream_side_inputs at 0x7c0b15feb490> ====================\n",
            "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function fix_side_input_pcoll_coders at 0x7c0b15feb5b0> ====================\n",
            "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function pack_combiners at 0x7c0b15febac0> ====================\n",
            "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function lift_combiners at 0x7c0b15febb50> ====================\n",
            "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function expand_sdf at 0x7c0b15febd00> ====================\n",
            "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function expand_gbk at 0x7c0b15febd90> ====================\n",
            "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function sink_flattens at 0x7c0b15febeb0> ====================\n",
            "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function greedily_fuse at 0x7c0b15febf40> ====================\n",
            "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function read_to_impulse at 0x7c0b15ff0040> ====================\n",
            "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function impulse_to_input at 0x7c0b15ff00d0> ====================\n",
            "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function sort_stages at 0x7c0b15ff0310> ====================\n",
            "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function add_impulse_to_dangling_transforms at 0x7c0b15ff0430> ====================\n",
            "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function setup_timer_mapping at 0x7c0b15ff0280> ====================\n",
            "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function populate_data_channel_coders at 0x7c0b15ff03a0> ====================\n",
            "INFO:apache_beam.runners.worker.statecache:Creating state cache with size 104857600\n",
            "INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:Created Worker handler <apache_beam.runners.portability.fn_api_runner.worker_handlers.EmbeddedWorkerHandler object at 0x7c0b163e7760> for environment ref_Environment_default_environment_1 (beam:env:embedded_python:v1, b'')\n",
            "INFO:apache_beam.runners.worker.statecache:Creating state cache with size 104857600\n",
            "INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:Created Worker handler <apache_beam.runners.portability.fn_api_runner.worker_handlers.EmbeddedWorkerHandler object at 0x7c0b16066ef0> for environment ref_Environment_default_environment_1 (beam:env:embedded_python:v1, b'')\n",
            "INFO:apache_beam.io.filebasedsink:Starting finalize_write threads with num_shards: 2 (skipped: 0), batches: 2, num_threads: 2\n",
            "INFO:apache_beam.io.filebasedsink:Renamed 2 shards in 0.00 seconds.\n"
          ]
        }
      ],
      "source": [
        "! python3 preprocess.py \\\n",
        "  --work-dir $WORK_DIR \\\n",
        "  --runner DirectRunner \\\n",
        "  --input $WORK_DIR/data.json \\\n",
        "  --output $WORK_DIR/transformed_data \\\n",
        "  --mode test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "geZM9Sbj45LK"
      },
      "source": [
        "## Segundo ejercicio\n",
        "\n",
        "Desarrollar una tarea de entrenamiento para los datos preprocesados. Requisitos:\n",
        "\n",
        "- Soportar ejecuciones en local usando el SDK de AI-Platform y ejecuciones en GCP con el mismo código."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OMUwXgm_5el-"
      },
      "source": [
        "Se crea el directorio donde trabajaremos:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "HMi8dI1gLoIc",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "%mkdir /content/batch/trainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3dJyMXTuNPwo",
        "outputId": "7670ec8c-ba85-4d86-869a-ad30cf8708d6",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing trainer/__init__.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile trainer/__init__.py\n",
        "\n",
        "version = \"0.1.0\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mUu6gKaTL_S2",
        "outputId": "7f605b87-cc49-49ce-c469-09ab92b1a1b2",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing trainer/task.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile trainer/task.py\n",
        "from __future__ import absolute_import\n",
        "\n",
        "import argparse\n",
        "import multiprocessing as mp\n",
        "import logging\n",
        "import tempfile\n",
        "import os\n",
        "\n",
        "import pickle\n",
        "import gensim\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import (\n",
        "    Dense,\n",
        "    Dropout,\n",
        "    Embedding,\n",
        "    LSTM,\n",
        ")\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "\n",
        "# WORD2VEC\n",
        "W2V_SIZE = 300 #tamaño del embedding\n",
        "W2V_WINDOW = 7 #tamaño de la ventana\n",
        "# 32\n",
        "W2V_EPOCH = 5 #epochs para entrrenar\n",
        "W2V_MIN_COUNT = 10 #se eliminan palabras que aparecen menos de 10 veces\n",
        "\n",
        "# KERAS\n",
        "SEQUENCE_LENGTH = 300 #longitud de secuencia para el padding\n",
        "\n",
        "# SENTIMENT definimos las etiquetas\n",
        "POSITIVE = \"POSITIVE\"\n",
        "NEGATIVE = \"NEGATIVE\"\n",
        "NEUTRAL = \"NEUTRAL\"\n",
        "SENTIMENT_THRESHOLDS = (0.4, 0.7) #todas las probabilidades entre 0.4 y 0.7 se asignan a neutral\n",
        "\n",
        "# EXPORT\n",
        "KERAS_MODEL = \"model.h5\" #nombre del archivo del modelo\n",
        "WORD2VEC_MODEL = \"model.w2v\" #nombre del archivo del embedding\n",
        "TOKENIZER_MODEL = \"tokenizer.pkl\" #nombre del tokenizador\n",
        "ENCODER_MODEL = \"encoder.pkl\" #nombre del encoder\n",
        "\n",
        "\n",
        "def generate_word2vec(train_df):\n",
        "  ### Genera el embedding y lo entrena\n",
        "    documents = [_text.split() for _text in train_df.text.values]\n",
        "    w2v_model = gensim.models.word2vec.Word2Vec(\n",
        "        vector_size=W2V_SIZE,\n",
        "        window=W2V_WINDOW,\n",
        "        min_count=W2V_MIN_COUNT,\n",
        "        workers=mp.cpu_count(),\n",
        "    )\n",
        "    w2v_model.build_vocab(documents)\n",
        "\n",
        "    words = w2v_model.wv.index_to_key\n",
        "    vocab_size = len(words)\n",
        "    logging.info(f\"Vocab size: {vocab_size}\")\n",
        "    w2v_model.train(documents, total_examples=len(documents), epochs=W2V_EPOCH)\n",
        "\n",
        "    return w2v_model\n",
        "\n",
        "\n",
        "def generate_tokenizer(train_df):\n",
        "  ### Genera nuestro tokenizador\n",
        "    tokenizer = Tokenizer()\n",
        "    tokenizer.fit_on_texts(train_df.text)\n",
        "    vocab_size = len(tokenizer.word_index) + 1\n",
        "    logging.info(f\"Total words: {vocab_size}\")\n",
        "    return tokenizer, vocab_size\n",
        "\n",
        "\n",
        "def generate_label_encoder(train_df):\n",
        "  ### COdifica las etiquetas (de texto a número)\n",
        "    encoder = LabelEncoder()\n",
        "    encoder.fit(train_df.sentiment.tolist())\n",
        "    return encoder\n",
        "\n",
        "\n",
        "def generate_embedding(word2vec_model, vocab_size, tokenizer):\n",
        "  ### Genera el embedding en base al Word2Vec\n",
        "    embedding_matrix = np.zeros((vocab_size, W2V_SIZE))\n",
        "    for word, i in tokenizer.word_index.items():\n",
        "        if word in word2vec_model.wv:\n",
        "            embedding_matrix[i] = word2vec_model.wv[word]\n",
        "    return Embedding(\n",
        "        vocab_size,\n",
        "        W2V_SIZE,\n",
        "        weights=[embedding_matrix],\n",
        "        input_length=SEQUENCE_LENGTH,\n",
        "        trainable=False,\n",
        "    )\n",
        "\n",
        "\n",
        "def train_and_evaluate(\n",
        "    work_dir, train_df, eval_df, batch_size=1024, epochs=8, steps=1000\n",
        "):\n",
        "\n",
        "    \"\"\"\n",
        "    Trains and evaluates the estimator given.\n",
        "    The input functions are generated by the preprocessing function.\n",
        "    \"\"\"\n",
        "\n",
        "    model_dir = os.path.join(work_dir, \"data/model\") # Comprobamos si ya existe un modelo\n",
        "\n",
        "    parent_dir = os.path.dirname(model_dir)\n",
        "    if not tf.io.gfile.exists(parent_dir):\n",
        "        tf.io.gfile.makedirs(parent_dir)\n",
        "\n",
        "    # Check if the model directory exists and remove it if it does\n",
        "    if tf.io.gfile.exists(model_dir):\n",
        "        tf.io.gfile.rmtree(model_dir)\n",
        "\n",
        "    # Create the model directory\n",
        "    tf.io.gfile.mkdir(model_dir)\n",
        "\n",
        "\n",
        "    # Configuramos donde guardar el modelo\n",
        "    run_config = tf.estimator.RunConfig()\n",
        "    run_config = run_config.replace(model_dir=model_dir)\n",
        "\n",
        "    # Nos permite seguir el entrenamiento cada 10 steps\n",
        "    run_config = run_config.replace(save_summary_steps=10)\n",
        "\n",
        "    # Generamos el Word2Vec para entrenamiento\n",
        "    logging.info(\"---- Generating word2vec model ----\")\n",
        "    word2vec_model = generate_word2vec(train_df)\n",
        "\n",
        "    # Generamos el tokenizador con el dato de entrenamiento e inferimos en el de entrenamiento y evaluación\n",
        "    logging.info(\"---- Generating tokenizer ----\")\n",
        "    tokenizer, vocab_size = generate_tokenizer(train_df)\n",
        "\n",
        "    logging.info(\"---- Tokenizing train data ----\")\n",
        "    x_train = pad_sequences(\n",
        "        tokenizer.texts_to_sequences(train_df.text), maxlen=SEQUENCE_LENGTH\n",
        "    ) #Añadimos el padding\n",
        "    logging.info(\"---- Tokenizing eval data ----\")\n",
        "    x_eval = pad_sequences(\n",
        "        tokenizer.texts_to_sequences(eval_df.text), maxlen=SEQUENCE_LENGTH\n",
        "    )\n",
        "\n",
        "    # Generamos los encodings de las etiquetas tanto para entrenamiento como para evaluación\n",
        "    logging.info(\"---- Generating label encoder ----\")\n",
        "    label_encoder = generate_label_encoder(train_df)\n",
        "\n",
        "    logging.info(\"---- Encoding train target ----\")\n",
        "    y_train = label_encoder.transform(train_df.sentiment.tolist())\n",
        "    logging.info(\"---- Encoding eval target ----\")\n",
        "    y_eval = label_encoder.transform(eval_df.sentiment.tolist())\n",
        "\n",
        "    y_train = y_train.reshape(-1, 1) #adaptamos la forma del tensor\n",
        "    y_eval = y_eval.reshape(-1, 1)\n",
        "\n",
        "    # Create Embedding Layer\n",
        "    logging.info(\"---- Generating embedding layer ----\")\n",
        "    embedding_layer = generate_embedding(word2vec_model, vocab_size, tokenizer) #capade embedding\n",
        "    # Construimos nuestra red LSTM\n",
        "    logging.info(\"---- Generating Sequential model ----\")\n",
        "    model = Sequential()\n",
        "    model.add(embedding_layer)\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
        "    model.add(Dense(1, activation=\"sigmoid\"))\n",
        "\n",
        "    model.summary()\n",
        "\n",
        "    logging.info(\"---- Adding loss function to model ----\")\n",
        "    model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"]) #problema binario y medimos según el accuracy\n",
        "\n",
        "    logging.info(\"---- Adding callbacks to model ----\")\n",
        "    callbacks = [\n",
        "        ReduceLROnPlateau(monitor=\"val_loss\", patience=5, cooldown=0),\n",
        "        EarlyStopping(monitor=\"val_accuracy\", min_delta=1e-4, patience=5), #early stopping para agilizar\n",
        "    ]\n",
        "\n",
        "    logging.info(\"---- Training model ----\")\n",
        "    model.fit(\n",
        "        x_train,\n",
        "        y_train,\n",
        "        batch_size=batch_size,\n",
        "        steps_per_epoch=steps,\n",
        "        epochs=epochs,\n",
        "        validation_split=0.1,\n",
        "        verbose=1,\n",
        "        callbacks=callbacks,\n",
        "    ) #entrenamos nuestro modelo\n",
        "\n",
        "    logging.info(\"---- Evaluating model ----\")\n",
        "    score = model.evaluate(x_eval, y_eval, batch_size=batch_size) #evaliamos el modelo\n",
        "    logging.info(f\"ACCURACY: {score[1]}\")\n",
        "    logging.info(f\"LOSS: {score[0]}\")\n",
        "\n",
        "    logging.info(\"---- Saving models ----\")\n",
        "    pickle.dump(\n",
        "        tokenizer,\n",
        "        tf.io.gfile.GFile(os.path.join(model_dir, TOKENIZER_MODEL), mode=\"wb\"),\n",
        "        protocol=0,\n",
        "    ) #guardamos el tokenizador\n",
        "    with tempfile.NamedTemporaryFile(suffix=\".h5\") as local_file:\n",
        "        with tf.io.gfile.GFile(\n",
        "            os.path.join(model_dir, KERAS_MODEL), mode=\"wb\" #guardamos el archivo\n",
        "        ) as gcs_file:\n",
        "            model.save(local_file.name)\n",
        "            gcs_file.write(local_file.read())\n",
        "\n",
        "    # word2vec_model.save(os.path.join(model_dir, WORD2VEC_MODEL))\n",
        "\n",
        "    # pickle.dump(\n",
        "    #     label_encoder, open(os.path.join(model_dir, ENCODER_MODEL), \"wb\"), protocol=0\n",
        "    # )\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    \"\"\"Main function called by AI Platform.\"\"\"\n",
        "\n",
        "    logging.getLogger().setLevel(logging.INFO) # Activamos el logger\n",
        "\n",
        "    parser = argparse.ArgumentParser( #Implementamos el parseador de argumentos. Ver script de preprocesamiento para más detalle\n",
        "        formatter_class=argparse.ArgumentDefaultsHelpFormatter\n",
        "    )\n",
        "\n",
        "    parser.add_argument(\n",
        "        \"--job-dir\",\n",
        "        help=\"Directory for staging trainer files. \"\n",
        "        \"This can be a Google Cloud Storage path.\",\n",
        "    )\n",
        "\n",
        "    parser.add_argument(\n",
        "        \"--work-dir\",\n",
        "        required=True,\n",
        "        help=\"Directory for staging and working files. \"\n",
        "        \"This can be a Google Cloud Storage path.\",\n",
        "    )\n",
        "\n",
        "    parser.add_argument(\n",
        "        \"--batch-size\",\n",
        "        type=int,\n",
        "        default=1024,\n",
        "        help=\"Batch size for training and evaluation.\",\n",
        "    )\n",
        "\n",
        "    parser.add_argument(\n",
        "        \"--epochs\", type=int, default=8, help=\"Number of epochs to train the model\",\n",
        "    )\n",
        "\n",
        "    parser.add_argument(\n",
        "        \"--steps\",\n",
        "        type=int,\n",
        "        default=1000,\n",
        "        help=\"Number of steps per epoch to train the model\",\n",
        "    )\n",
        "\n",
        "    args = parser.parse_args() #cargamos todos los argumentos antes mencionados\n",
        "\n",
        "    train_data_files = tf.io.gfile.glob(\n",
        "        os.path.join(args.work_dir, \"transformed_data/train/part-*\") # el * permite que lea todas las particiones\n",
        "    ) #archivos de entrenamiento\n",
        "    eval_data_files = tf.io.gfile.glob(\n",
        "        os.path.join(args.work_dir, \"transformed_data/eval/part-*\")\n",
        "    ) #archivos de test\n",
        "\n",
        "    train_df = pd.concat(\n",
        "        [\n",
        "            pd.read_csv(\n",
        "                f,\n",
        "                names=[\"text\", \"sentiment\"],\n",
        "                dtype={\"text\": \"string\", \"sentiment\": \"string\"},\n",
        "            ) # leemos cada csv\n",
        "            for f in train_data_files\n",
        "        ]\n",
        "    ).dropna() #generamos un dataframe con todas las particiones\n",
        "\n",
        "    eval_df = pd.concat(\n",
        "        [\n",
        "            pd.read_csv(\n",
        "                f,\n",
        "                names=[\"text\", \"sentiment\"],\n",
        "                dtype={\"text\": \"string\", \"sentiment\": \"string\"},\n",
        "            )\n",
        "            for f in eval_data_files\n",
        "        ]\n",
        "    ).dropna()\n",
        "\n",
        "\n",
        "    train_and_evaluate(\n",
        "        args.work_dir,\n",
        "        train_df=train_df,\n",
        "        eval_df=eval_df,\n",
        "        batch_size=args.batch_size,\n",
        "        epochs=args.epochs,\n",
        "        steps=args.steps,\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8axPHdHX6d9W"
      },
      "source": [
        "### Validación Train en local\n",
        "\n",
        "Con el comando mostrado a continuación se valida el correcto entrenamiento del modelo usando los datos preprocesados del apartado anterior."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m9997ZzsLmq-",
        "outputId": "798d6063-c848-49d7-bf7e-7fffaf2a37c1",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updated property [ml_engine/local_python].\n",
            "2023-09-17 15:32:27.805840: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-09-17 15:32:28.754455: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "WARNING:tensorflow:From /content/batch/trainer/task.py:125: RunConfig.__init__ (from tensorflow_estimator.python.estimator.run_config) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.keras instead.\n",
            "INFO:tensorflow:TF_CONFIG environment variable: {'job': {'job_name': 'trainer.task', 'args': ['--work-dir', '/content/batch', '--epochs', '1']}, 'task': {}, 'cluster': {}, 'environment': 'cloud'}\n",
            "INFO:root:---- Generating word2vec model ----\n",
            "INFO:gensim.utils:Word2Vec lifecycle event {'params': 'Word2Vec<vocab=0, vector_size=300, alpha=0.025>', 'datetime': '2023-09-17T15:32:30.079691', 'gensim': '4.3.2', 'python': '3.10.12 (main, Jun 11 2023, 05:26:28) [GCC 11.4.0]', 'platform': 'Linux-5.15.109+-x86_64-with-glibc2.35', 'event': 'created'}\n",
            "INFO:gensim.models.word2vec:collecting all words and their counts\n",
            "INFO:gensim.models.word2vec:PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
            "INFO:gensim.models.word2vec:PROGRESS: at sentence #10000, processed 68005 words, keeping 11719 word types\n",
            "INFO:gensim.models.word2vec:collected 14769 word types from a corpus of 114195 raw words and 15922 sentences\n",
            "INFO:gensim.models.word2vec:Creating a fresh vocabulary\n",
            "INFO:gensim.utils:Word2Vec lifecycle event {'msg': 'effective_min_count=10 retains 1608 unique words (10.89% of original 14769, drops 13161)', 'datetime': '2023-09-17T15:32:30.118706', 'gensim': '4.3.2', 'python': '3.10.12 (main, Jun 11 2023, 05:26:28) [GCC 11.4.0]', 'platform': 'Linux-5.15.109+-x86_64-with-glibc2.35', 'event': 'prepare_vocab'}\n",
            "INFO:gensim.utils:Word2Vec lifecycle event {'msg': 'effective_min_count=10 leaves 84640 word corpus (74.12% of original 114195, drops 29555)', 'datetime': '2023-09-17T15:32:30.118834', 'gensim': '4.3.2', 'python': '3.10.12 (main, Jun 11 2023, 05:26:28) [GCC 11.4.0]', 'platform': 'Linux-5.15.109+-x86_64-with-glibc2.35', 'event': 'prepare_vocab'}\n",
            "INFO:gensim.models.word2vec:deleting the raw counts dictionary of 14769 items\n",
            "INFO:gensim.models.word2vec:sample=0.001 downsamples 57 most-common words\n",
            "INFO:gensim.utils:Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 70426.49373797781 word corpus (83.2%% of prior 84640)', 'datetime': '2023-09-17T15:32:30.126106', 'gensim': '4.3.2', 'python': '3.10.12 (main, Jun 11 2023, 05:26:28) [GCC 11.4.0]', 'platform': 'Linux-5.15.109+-x86_64-with-glibc2.35', 'event': 'prepare_vocab'}\n",
            "INFO:gensim.models.word2vec:estimated required memory for 1608 words and 300 dimensions: 4663200 bytes\n",
            "INFO:gensim.models.word2vec:resetting layer weights\n",
            "INFO:gensim.utils:Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2023-09-17T15:32:30.140469', 'gensim': '4.3.2', 'python': '3.10.12 (main, Jun 11 2023, 05:26:28) [GCC 11.4.0]', 'platform': 'Linux-5.15.109+-x86_64-with-glibc2.35', 'event': 'build_vocab'}\n",
            "INFO:root:Vocab size: 1608\n",
            "INFO:gensim.utils:Word2Vec lifecycle event {'msg': 'training model with 2 workers on 1608 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=7 shrink_windows=True', 'datetime': '2023-09-17T15:32:30.140689', 'gensim': '4.3.2', 'python': '3.10.12 (main, Jun 11 2023, 05:26:28) [GCC 11.4.0]', 'platform': 'Linux-5.15.109+-x86_64-with-glibc2.35', 'event': 'train'}\n",
            "INFO:gensim.models.word2vec:EPOCH 0: training on 114195 raw words (70458 effective words) took 0.1s, 506898 effective words/s\n",
            "INFO:gensim.models.word2vec:EPOCH 1: training on 114195 raw words (70380 effective words) took 0.1s, 495994 effective words/s\n",
            "INFO:gensim.models.word2vec:EPOCH 2: training on 114195 raw words (70406 effective words) took 0.1s, 513542 effective words/s\n",
            "INFO:gensim.models.word2vec:EPOCH 3: training on 114195 raw words (70520 effective words) took 0.1s, 538024 effective words/s\n",
            "INFO:gensim.models.word2vec:EPOCH 4: training on 114195 raw words (70491 effective words) took 0.1s, 500031 effective words/s\n",
            "INFO:gensim.utils:Word2Vec lifecycle event {'msg': 'training on 570975 raw words (352255 effective words) took 0.7s, 487974 effective words/s', 'datetime': '2023-09-17T15:32:30.862666', 'gensim': '4.3.2', 'python': '3.10.12 (main, Jun 11 2023, 05:26:28) [GCC 11.4.0]', 'platform': 'Linux-5.15.109+-x86_64-with-glibc2.35', 'event': 'train'}\n",
            "INFO:root:---- Generating tokenizer ----\n",
            "INFO:root:Total words: 14770\n",
            "INFO:root:---- Tokenizing train data ----\n",
            "INFO:root:---- Tokenizing eval data ----\n",
            "INFO:root:---- Generating label encoder ----\n",
            "INFO:root:---- Encoding train target ----\n",
            "INFO:root:---- Encoding eval target ----\n",
            "INFO:root:---- Generating embedding layer ----\n",
            "INFO:root:---- Generating Sequential model ----\n",
            "2023-09-17 15:32:31.522465: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-09-17 15:32:32.123220: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-09-17 15:32:32.123534: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-09-17 15:32:32.124396: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-09-17 15:32:32.124664: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-09-17 15:32:32.124880: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-09-17 15:32:34.751758: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-09-17 15:32:34.752128: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-09-17 15:32:34.752356: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-09-17 15:32:34.752518: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:47] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2023-09-17 15:32:34.752626: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13692 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\n",
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 300, 300)          4431000   \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 300, 300)          0         \n",
            "                                                                 \n",
            " lstm (LSTM)                 (None, 100)               160400    \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1)                 101       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4591501 (17.52 MB)\n",
            "Trainable params: 160501 (626.96 KB)\n",
            "Non-trainable params: 4431000 (16.90 MB)\n",
            "_________________________________________________________________\n",
            "INFO:root:---- Adding loss function to model ----\n",
            "INFO:root:---- Adding callbacks to model ----\n",
            "INFO:root:---- Training model ----\n",
            "2023-09-17 15:32:42.824296: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x595f9dd7c920 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "2023-09-17 15:32:42.824337: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\n",
            "2023-09-17 15:32:42.994131: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
            "2023-09-17 15:32:43.608452: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8900\n",
            "2023-09-17 15:32:44.015923: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
            "  14/1000 [..............................] - ETA: 21:33 - loss: 0.6892 - accuracy: 0.56342023-09-17 15:33:02.009054: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 5429762589764948669\n",
            "2023-09-17 15:33:02.009142: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 11868125375404721925\n",
            "2023-09-17 15:33:02.009164: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 9511102316410498682\n",
            "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 1000 batches). You may need to use the repeat() function when building your dataset.\n",
            "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 1000 batches). You may need to use the repeat() function when building your dataset.\n",
            "1000/1000 [==============================] - 27s 18ms/step - loss: 0.6892 - accuracy: 0.5634 - val_loss: 0.6459 - val_accuracy: 0.7137 - lr: 0.0010\n",
            "INFO:root:---- Evaluating model ----\n",
            "4/4 [==============================] - 1s 139ms/step - loss: 0.6750 - accuracy: 0.5962\n",
            "INFO:root:ACCURACY: 0.5961635708808899\n",
            "INFO:root:LOSS: 0.6749994158744812\n",
            "INFO:root:---- Saving models ----\n",
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ],
      "source": [
        "# Explicitly tell `gcloud ai-platform local train` to use Python 3\n",
        "! gcloud config set ml_engine/local_python $(which python3)\n",
        "\n",
        "# This is similar to `python -m trainer.task --job-dir local-training-output`\n",
        "# but it better replicates the AI Platform environment, especially for\n",
        "# distributed training (not applicable here).\n",
        "! gcloud ai-platform local train \\\n",
        "  --package-path trainer \\\n",
        "  --module-name trainer.task \\\n",
        "  -- \\\n",
        "  --work-dir $WORK_DIR \\\n",
        "  --epochs 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0nhlkcjn62Zo"
      },
      "source": [
        "## Tercer ejercicio\n",
        "\n",
        "Desarrollar un pipeline de inferencia utilizando Apache Beam para generar predicciones usando los modelos generados en el apartado anterior así como los datos de test generados en el primer ejercicio.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eHBpLXB-OmjT",
        "outputId": "07ad20a8-3030-4039-b225-2ce0417b20ff",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing predict.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile predict.py\n",
        "\n",
        "\n",
        "from __future__ import absolute_import\n",
        "from __future__ import print_function\n",
        "\n",
        "import argparse\n",
        "import tempfile\n",
        "import json\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "\n",
        "import apache_beam as beam\n",
        "from apache_beam.io import ReadFromText\n",
        "from apache_beam.io import WriteToText\n",
        "from apache_beam.options.pipeline_options import GoogleCloudOptions\n",
        "from apache_beam.options.pipeline_options import PipelineOptions\n",
        "from apache_beam.options.pipeline_options import SetupOptions\n",
        "from apache_beam.coders.coders import Coder\n",
        "\n",
        "import pickle\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# KERAS\n",
        "SEQUENCE_LENGTH = 300\n",
        "\n",
        "# SENTIMENT\n",
        "POSITIVE = \"TROLL\"\n",
        "NEGATIVE = \"NO TROLL\"\n",
        "#NEUTRAL = \"NEUTRAL\"\n",
        "#SENTIMENT_THRESHOLDS = (0.4, 0.7)\n",
        "\n",
        "# EXPORT\n",
        "KERAS_MODEL = \"model.h5\"\n",
        "TOKENIZER_MODEL = \"tokenizer.pkl\"\n",
        "\n",
        "\n",
        "class Predict(beam.DoFn): #realiza predicciones y hereda de Beam\n",
        "    def __init__(\n",
        "        self, model_dir,\n",
        "    ):\n",
        "        self.model_dir = model_dir\n",
        "        self.model = None\n",
        "        self.tokenizer = None\n",
        "\n",
        "    def setup(self): #es un método de apache Beam que se ejecuta la primera vez que se ejecute e inicializa el modelo de Keras y el tokenizador cargándolos\n",
        "        keras_model_path = os.path.join(self.model_dir, KERAS_MODEL)\n",
        "        with tempfile.NamedTemporaryFile(suffix=\".h5\") as local_file:\n",
        "            with tf.io.gfile.GFile(keras_model_path, mode=\"rb\") as gcs_file:\n",
        "                local_file.write(gcs_file.read())\n",
        "                self.model = tf.keras.models.load_model(local_file.name)\n",
        "\n",
        "        tokenizer_path = os.path.join(self.model_dir, TOKENIZER_MODEL)\n",
        "        self.tokenizer = pickle.load(tf.io.gfile.GFile(tokenizer_path, mode=\"rb\"))\n",
        "\n",
        "    def decode_sentiment(self, score, include_neutral= False):\n",
        "\n",
        "        return NEGATIVE if score < 0.5 else POSITIVE\n",
        "\n",
        "    def process(self, element): #recoge elt exto a predecir, tokeniza\n",
        "        start_at = time.time()\n",
        "        # Tokenize text\n",
        "        x_test = pad_sequences(\n",
        "            self.tokenizer.texts_to_sequences([element]), maxlen=SEQUENCE_LENGTH\n",
        "        ) #tokenizado y padding\n",
        "        # Predict\n",
        "        score = self.model.predict([x_test])[0] #realiza la inferencia\n",
        "        # Decode sentiment\n",
        "        label = self.decode_sentiment(score) #trasnforma la predicción en sentimiento\n",
        "\n",
        "        yield {\n",
        "            \"text\": element,\n",
        "            \"label\": label,\n",
        "            \"score\": float(score),\n",
        "            \"elapsed_time\": time.time() - start_at,\n",
        "        } #devuelve un diccionario con la información clave\n",
        "\n",
        "\n",
        "class CustomCoder(Coder):\n",
        "    \"\"\"A custom coder used for reading and writing strings\"\"\"\n",
        "\n",
        "    def __init__(self, encoding: str):\n",
        "        # latin-1\n",
        "        # iso-8859-1\n",
        "        self.enconding = encoding\n",
        "\n",
        "    def encode(self, value):\n",
        "        return value.encode(self.enconding)\n",
        "\n",
        "    def decode(self, value):\n",
        "        return value.decode(self.enconding)\n",
        "\n",
        "    def is_deterministic(self):\n",
        "        return True\n",
        "\n",
        "\n",
        "def run(model_dir, source, sink, beam_options=None):\n",
        "    with beam.Pipeline(options=beam_options) as p:\n",
        "        _ = (\n",
        "            p #generamos el pipeline\n",
        "            | \"Read data\" >> source #lectura de datos\n",
        "            | \"Predict\" >> beam.ParDo(Predict(model_dir)) #hacemos la predicción\n",
        "            | \"Format as JSON\" >> beam.Map(json.dumps) #convertimos el diccionario en string\n",
        "            | \"Write predictions\" >> sink #guardamos la información\n",
        "        )\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    \"\"\"Main function\"\"\"\n",
        "    parser = argparse.ArgumentParser(\n",
        "        formatter_class=argparse.ArgumentDefaultsHelpFormatter # parseador de argumentos\n",
        "    )\n",
        "\n",
        "    parser.add_argument( #directorio de trabajo\n",
        "        \"--work-dir\",\n",
        "        dest=\"work_dir\",\n",
        "        required=True,\n",
        "        help=\"Directory for temporary files and preprocessed datasets to. \"\n",
        "        \"This can be a Google Cloud Storage path.\",\n",
        "    )\n",
        "\n",
        "    parser.add_argument( #ubicación del modelo para inferencia\n",
        "        \"--model-dir\",\n",
        "        dest=\"model_dir\",\n",
        "        required=True,\n",
        "        help=\"Path to the exported TensorFlow model. \"\n",
        "        \"This can be a Google Cloud Storage path.\",\n",
        "    )\n",
        "\n",
        "    verbs = parser.add_subparsers(dest=\"verb\")\n",
        "    batch_verb = verbs.add_parser(\"batch\", help=\"Batch prediction\")\n",
        "    batch_verb.add_argument( #datos de entrada para la predicción\n",
        "        \"--inputs-dir\",\n",
        "        dest=\"inputs_dir\",\n",
        "        required=True,\n",
        "        help=\"Input directory where CSV data files are read from. \"\n",
        "        \"This can be a Google Cloud Storage path.\",\n",
        "    )\n",
        "    batch_verb.add_argument( #datos de salida para la predicción\n",
        "        \"--outputs-dir\",\n",
        "        dest=\"outputs_dir\",\n",
        "        required=True,\n",
        "        help=\"Directory to store prediction results. \"\n",
        "        \"This can be a Google Cloud Storage path.\",\n",
        "    )\n",
        "\n",
        "    args, pipeline_args = parser.parse_known_args()\n",
        "    print(args)\n",
        "    beam_options = PipelineOptions(pipeline_args)\n",
        "    beam_options.view_as(SetupOptions).save_main_session = True\n",
        "    # beam_options.view_as(DirectOptions).direct_num_workers = 0\n",
        "\n",
        "    project = beam_options.view_as(GoogleCloudOptions).project\n",
        "\n",
        "    if args.verb == \"batch\": #si es batch genera transformadores iniciales, la fuente y la salida de datos\n",
        "        results_prefix = os.path.join(args.outputs_dir, \"part\")\n",
        "\n",
        "        source = ReadFromText(args.inputs_dir, coder=CustomCoder(\"latin-1\"))\n",
        "        sink = WriteToText(results_prefix)\n",
        "\n",
        "    else: # de momento no funciona si no es batch\n",
        "        parser.print_usage()\n",
        "        sys.exit(1)\n",
        "\n",
        "    run(args.model_dir, source, sink, beam_options)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MUdgcLIP7a42"
      },
      "source": [
        "Generamos un timestamp para la ejecución de las predicciones"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "_pvKMtuQPOlr",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "from datetime import datetime\n",
        "\n",
        "# current date and time\n",
        "TIMESTAMP = datetime.now().strftime('%Y-%m-%d_%H-%M-%S')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RkLMdMup70ne"
      },
      "source": [
        "### Validación Predict en local\n",
        "\n",
        "Con el comando mostrado a continuación se valida la correcta inferencia usando los modelos anteriores y los datos de test generados anteriormente."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iUJN0XCLPR_X",
        "outputId": "9858b61f-df34-47d3-aa60-5d63394a646d",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-09-17 15:34:49.940588: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-09-17 15:34:50.716765: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Namespace(work_dir='/content/batch', model_dir='/content/batch/data/model', verb='batch', inputs_dir='/content/batch/transformed_data/test/part-00000-of-00002', outputs_dir='/content/batch/predictions/2023-09-17_15-34-41')\n",
            "WARNING:apache_beam.options.pipeline_options:Discarding unparseable args: ['/content/batch/transformed_data/test/part-00001-of-00002']\n",
            "2023-09-17 15:34:52.807201: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-09-17 15:34:52.838808: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-09-17 15:34:52.839096: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-09-17 15:34:52.839822: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-09-17 15:34:52.840269: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-09-17 15:34:52.840526: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-09-17 15:34:53.759001: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-09-17 15:34:53.759321: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-09-17 15:34:53.759529: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-09-17 15:34:53.759688: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:47] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2023-09-17 15:34:53.759738: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13692 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\n",
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 0s 132ms/step\n",
            "1/1 [==============================] - 0s 127ms/step\n",
            "1/1 [==============================] - 0s 156ms/step\n",
            "1/1 [==============================] - 0s 132ms/step\n",
            "1/1 [==============================] - 0s 132ms/step\n",
            "1/1 [==============================] - 0s 140ms/step\n",
            "1/1 [==============================] - 0s 153ms/step\n",
            "1/1 [==============================] - 0s 154ms/step\n",
            "1/1 [==============================] - 0s 153ms/step\n",
            "1/1 [==============================] - 0s 145ms/step\n",
            "1/1 [==============================] - 0s 142ms/step\n",
            "1/1 [==============================] - 0s 147ms/step\n",
            "1/1 [==============================] - 0s 165ms/step\n",
            "1/1 [==============================] - 0s 146ms/step\n",
            "1/1 [==============================] - 0s 153ms/step\n",
            "1/1 [==============================] - 0s 141ms/step\n",
            "1/1 [==============================] - 0s 146ms/step\n",
            "1/1 [==============================] - 0s 151ms/step\n",
            "1/1 [==============================] - 0s 147ms/step\n",
            "1/1 [==============================] - 0s 164ms/step\n",
            "1/1 [==============================] - 0s 145ms/step\n",
            "1/1 [==============================] - 0s 122ms/step\n",
            "1/1 [==============================] - 0s 81ms/step\n",
            "1/1 [==============================] - 0s 82ms/step\n",
            "1/1 [==============================] - 0s 87ms/step\n",
            "1/1 [==============================] - 0s 85ms/step\n",
            "1/1 [==============================] - 0s 82ms/step\n",
            "1/1 [==============================] - 0s 99ms/step\n",
            "1/1 [==============================] - 0s 83ms/step\n",
            "1/1 [==============================] - 0s 81ms/step\n",
            "1/1 [==============================] - 0s 82ms/step\n",
            "1/1 [==============================] - 0s 87ms/step\n",
            "1/1 [==============================] - 0s 88ms/step\n",
            "1/1 [==============================] - 0s 84ms/step\n",
            "1/1 [==============================] - 0s 99ms/step\n",
            "1/1 [==============================] - 0s 84ms/step\n",
            "1/1 [==============================] - 0s 83ms/step\n",
            "1/1 [==============================] - 0s 90ms/step\n",
            "1/1 [==============================] - 0s 82ms/step\n",
            "1/1 [==============================] - 0s 92ms/step\n",
            "1/1 [==============================] - 0s 82ms/step\n",
            "1/1 [==============================] - 0s 93ms/step\n",
            "1/1 [==============================] - 0s 85ms/step\n",
            "1/1 [==============================] - 0s 87ms/step\n",
            "1/1 [==============================] - 0s 87ms/step\n",
            "1/1 [==============================] - 0s 84ms/step\n",
            "1/1 [==============================] - 0s 92ms/step\n",
            "1/1 [==============================] - 0s 92ms/step\n",
            "1/1 [==============================] - 0s 84ms/step\n",
            "1/1 [==============================] - 0s 92ms/step\n",
            "1/1 [==============================] - 0s 110ms/step\n",
            "1/1 [==============================] - 0s 97ms/step\n",
            "1/1 [==============================] - 0s 139ms/step\n",
            "1/1 [==============================] - 0s 126ms/step\n",
            "1/1 [==============================] - 0s 104ms/step\n",
            "1/1 [==============================] - 0s 102ms/step\n",
            "1/1 [==============================] - 0s 87ms/step\n",
            "1/1 [==============================] - 0s 86ms/step\n",
            "1/1 [==============================] - 0s 88ms/step\n",
            "1/1 [==============================] - 0s 100ms/step\n",
            "1/1 [==============================] - 0s 85ms/step\n",
            "1/1 [==============================] - 0s 86ms/step\n",
            "1/1 [==============================] - 0s 83ms/step\n",
            "1/1 [==============================] - 0s 93ms/step\n",
            "1/1 [==============================] - 0s 107ms/step\n",
            "1/1 [==============================] - 0s 94ms/step\n",
            "1/1 [==============================] - 0s 86ms/step\n",
            "1/1 [==============================] - 0s 83ms/step\n",
            "1/1 [==============================] - 0s 88ms/step\n",
            "1/1 [==============================] - 0s 104ms/step\n",
            "1/1 [==============================] - 0s 88ms/step\n",
            "1/1 [==============================] - 0s 83ms/step\n",
            "1/1 [==============================] - 0s 85ms/step\n",
            "1/1 [==============================] - 0s 103ms/step\n",
            "1/1 [==============================] - 0s 85ms/step\n",
            "1/1 [==============================] - 0s 86ms/step\n",
            "1/1 [==============================] - 0s 86ms/step\n",
            "1/1 [==============================] - 0s 88ms/step\n",
            "1/1 [==============================] - 0s 81ms/step\n",
            "1/1 [==============================] - 0s 83ms/step\n",
            "1/1 [==============================] - 0s 84ms/step\n",
            "1/1 [==============================] - 0s 83ms/step\n",
            "1/1 [==============================] - 0s 89ms/step\n",
            "1/1 [==============================] - 0s 82ms/step\n",
            "1/1 [==============================] - 0s 82ms/step\n",
            "1/1 [==============================] - 0s 89ms/step\n",
            "1/1 [==============================] - 0s 84ms/step\n",
            "1/1 [==============================] - 0s 87ms/step\n",
            "1/1 [==============================] - 0s 85ms/step\n",
            "1/1 [==============================] - 0s 83ms/step\n",
            "1/1 [==============================] - 0s 93ms/step\n",
            "1/1 [==============================] - 0s 87ms/step\n",
            "1/1 [==============================] - 0s 87ms/step\n",
            "1/1 [==============================] - 0s 86ms/step\n",
            "1/1 [==============================] - 0s 84ms/step\n",
            "1/1 [==============================] - 0s 83ms/step\n",
            "1/1 [==============================] - 0s 81ms/step\n",
            "1/1 [==============================] - 0s 86ms/step\n",
            "1/1 [==============================] - 0s 81ms/step\n",
            "1/1 [==============================] - 0s 89ms/step\n",
            "1/1 [==============================] - 0s 96ms/step\n",
            "1/1 [==============================] - 0s 136ms/step\n",
            "1/1 [==============================] - 0s 133ms/step\n",
            "1/1 [==============================] - 0s 131ms/step\n",
            "1/1 [==============================] - 0s 125ms/step\n",
            "1/1 [==============================] - 0s 155ms/step\n",
            "1/1 [==============================] - 0s 136ms/step\n",
            "1/1 [==============================] - 0s 134ms/step\n",
            "1/1 [==============================] - 0s 129ms/step\n",
            "1/1 [==============================] - 0s 128ms/step\n",
            "1/1 [==============================] - 0s 134ms/step\n",
            "1/1 [==============================] - 0s 145ms/step\n",
            "1/1 [==============================] - 0s 137ms/step\n",
            "1/1 [==============================] - 0s 142ms/step\n",
            "1/1 [==============================] - 0s 153ms/step\n",
            "1/1 [==============================] - 0s 145ms/step\n",
            "1/1 [==============================] - 0s 141ms/step\n",
            "1/1 [==============================] - 0s 154ms/step\n",
            "1/1 [==============================] - 0s 141ms/step\n",
            "1/1 [==============================] - 0s 150ms/step\n",
            "1/1 [==============================] - 0s 151ms/step\n",
            "1/1 [==============================] - 0s 144ms/step\n",
            "1/1 [==============================] - 0s 138ms/step\n",
            "1/1 [==============================] - 0s 146ms/step\n",
            "1/1 [==============================] - 0s 144ms/step\n",
            "1/1 [==============================] - 0s 146ms/step\n",
            "1/1 [==============================] - 0s 140ms/step\n",
            "1/1 [==============================] - 0s 159ms/step\n",
            "1/1 [==============================] - 0s 120ms/step\n",
            "1/1 [==============================] - 0s 82ms/step\n",
            "1/1 [==============================] - 0s 79ms/step\n",
            "1/1 [==============================] - 0s 83ms/step\n",
            "1/1 [==============================] - 0s 99ms/step\n",
            "1/1 [==============================] - 0s 85ms/step\n",
            "1/1 [==============================] - 0s 83ms/step\n",
            "1/1 [==============================] - 0s 84ms/step\n",
            "1/1 [==============================] - 0s 84ms/step\n",
            "1/1 [==============================] - 0s 85ms/step\n",
            "1/1 [==============================] - 0s 86ms/step\n",
            "1/1 [==============================] - 0s 84ms/step\n",
            "1/1 [==============================] - 0s 91ms/step\n",
            "1/1 [==============================] - 0s 81ms/step\n",
            "1/1 [==============================] - 0s 80ms/step\n",
            "1/1 [==============================] - 0s 101ms/step\n",
            "1/1 [==============================] - 0s 84ms/step\n",
            "1/1 [==============================] - 0s 84ms/step\n",
            "1/1 [==============================] - 0s 89ms/step\n",
            "1/1 [==============================] - 0s 85ms/step\n",
            "1/1 [==============================] - 0s 83ms/step\n",
            "1/1 [==============================] - 0s 82ms/step\n",
            "1/1 [==============================] - 0s 89ms/step\n",
            "1/1 [==============================] - 0s 82ms/step\n",
            "1/1 [==============================] - 0s 89ms/step\n",
            "1/1 [==============================] - 0s 89ms/step\n",
            "1/1 [==============================] - 0s 86ms/step\n",
            "1/1 [==============================] - 0s 85ms/step\n",
            "1/1 [==============================] - 0s 87ms/step\n",
            "1/1 [==============================] - 0s 81ms/step\n",
            "1/1 [==============================] - 0s 92ms/step\n",
            "1/1 [==============================] - 0s 81ms/step\n",
            "1/1 [==============================] - 0s 98ms/step\n",
            "1/1 [==============================] - 0s 91ms/step\n",
            "1/1 [==============================] - 0s 83ms/step\n",
            "1/1 [==============================] - 0s 82ms/step\n",
            "1/1 [==============================] - 0s 84ms/step\n",
            "1/1 [==============================] - 0s 83ms/step\n",
            "1/1 [==============================] - 0s 87ms/step\n",
            "1/1 [==============================] - 0s 82ms/step\n",
            "1/1 [==============================] - 0s 82ms/step\n",
            "1/1 [==============================] - 0s 97ms/step\n",
            "1/1 [==============================] - 0s 85ms/step\n",
            "1/1 [==============================] - 0s 85ms/step\n",
            "1/1 [==============================] - 0s 88ms/step\n",
            "1/1 [==============================] - 0s 83ms/step\n",
            "1/1 [==============================] - 0s 85ms/step\n",
            "1/1 [==============================] - 0s 83ms/step\n",
            "1/1 [==============================] - 0s 83ms/step\n",
            "1/1 [==============================] - 0s 106ms/step\n",
            "1/1 [==============================] - 0s 85ms/step\n",
            "1/1 [==============================] - 0s 85ms/step\n",
            "1/1 [==============================] - 0s 86ms/step\n",
            "1/1 [==============================] - 0s 86ms/step\n",
            "1/1 [==============================] - 0s 87ms/step\n",
            "1/1 [==============================] - 0s 84ms/step\n",
            "1/1 [==============================] - 0s 85ms/step\n",
            "1/1 [==============================] - 0s 84ms/step\n",
            "1/1 [==============================] - 0s 100ms/step\n",
            "1/1 [==============================] - 0s 83ms/step\n",
            "1/1 [==============================] - 0s 82ms/step\n",
            "1/1 [==============================] - 0s 91ms/step\n",
            "1/1 [==============================] - 0s 86ms/step\n",
            "1/1 [==============================] - 0s 89ms/step\n",
            "1/1 [==============================] - 0s 82ms/step\n",
            "1/1 [==============================] - 0s 93ms/step\n",
            "1/1 [==============================] - 0s 91ms/step\n",
            "1/1 [==============================] - 0s 85ms/step\n",
            "1/1 [==============================] - 0s 88ms/step\n",
            "1/1 [==============================] - 0s 84ms/step\n",
            "1/1 [==============================] - 0s 101ms/step\n",
            "1/1 [==============================] - 0s 84ms/step\n",
            "1/1 [==============================] - 0s 82ms/step\n",
            "1/1 [==============================] - 0s 90ms/step\n",
            "1/1 [==============================] - 0s 92ms/step\n",
            "1/1 [==============================] - 0s 91ms/step\n",
            "1/1 [==============================] - 0s 86ms/step\n",
            "1/1 [==============================] - 0s 88ms/step\n",
            "1/1 [==============================] - 0s 87ms/step\n",
            "1/1 [==============================] - 0s 84ms/step\n",
            "1/1 [==============================] - 0s 91ms/step\n",
            "1/1 [==============================] - 0s 81ms/step\n",
            "1/1 [==============================] - 0s 85ms/step\n",
            "1/1 [==============================] - 0s 121ms/step\n",
            "1/1 [==============================] - 0s 130ms/step\n",
            "1/1 [==============================] - 0s 137ms/step\n",
            "1/1 [==============================] - 0s 158ms/step\n",
            "1/1 [==============================] - 0s 135ms/step\n",
            "1/1 [==============================] - 0s 147ms/step\n",
            "1/1 [==============================] - 0s 142ms/step\n",
            "1/1 [==============================] - 0s 132ms/step\n",
            "1/1 [==============================] - 0s 141ms/step\n",
            "1/1 [==============================] - 0s 139ms/step\n",
            "1/1 [==============================] - 0s 140ms/step\n",
            "1/1 [==============================] - 0s 150ms/step\n",
            "1/1 [==============================] - 0s 139ms/step\n",
            "1/1 [==============================] - 0s 159ms/step\n",
            "1/1 [==============================] - 0s 152ms/step\n",
            "1/1 [==============================] - 0s 159ms/step\n",
            "1/1 [==============================] - 0s 150ms/step\n",
            "1/1 [==============================] - 0s 140ms/step\n",
            "1/1 [==============================] - 0s 152ms/step\n",
            "1/1 [==============================] - 0s 143ms/step\n",
            "1/1 [==============================] - 0s 144ms/step\n",
            "1/1 [==============================] - 0s 138ms/step\n",
            "1/1 [==============================] - 0s 144ms/step\n",
            "1/1 [==============================] - 0s 142ms/step\n",
            "1/1 [==============================] - 0s 149ms/step\n",
            "1/1 [==============================] - 0s 154ms/step\n",
            "1/1 [==============================] - 0s 153ms/step\n",
            "1/1 [==============================] - 0s 145ms/step\n",
            "1/1 [==============================] - 0s 127ms/step\n",
            "1/1 [==============================] - 0s 83ms/step\n",
            "1/1 [==============================] - 0s 85ms/step\n",
            "1/1 [==============================] - 0s 87ms/step\n",
            "1/1 [==============================] - 0s 84ms/step\n",
            "1/1 [==============================] - 0s 87ms/step\n",
            "1/1 [==============================] - 0s 91ms/step\n",
            "1/1 [==============================] - 0s 89ms/step\n",
            "1/1 [==============================] - 0s 86ms/step\n",
            "1/1 [==============================] - 0s 87ms/step\n",
            "1/1 [==============================] - 0s 82ms/step\n",
            "1/1 [==============================] - 0s 86ms/step\n",
            "1/1 [==============================] - 0s 87ms/step\n",
            "1/1 [==============================] - 0s 86ms/step\n",
            "1/1 [==============================] - 0s 83ms/step\n",
            "1/1 [==============================] - 0s 89ms/step\n",
            "1/1 [==============================] - 0s 85ms/step\n",
            "1/1 [==============================] - 0s 84ms/step\n",
            "1/1 [==============================] - 0s 83ms/step\n",
            "1/1 [==============================] - 0s 84ms/step\n",
            "1/1 [==============================] - 0s 86ms/step\n",
            "1/1 [==============================] - 0s 88ms/step\n",
            "1/1 [==============================] - 0s 83ms/step\n",
            "1/1 [==============================] - 0s 83ms/step\n",
            "1/1 [==============================] - 0s 87ms/step\n",
            "1/1 [==============================] - 0s 84ms/step\n",
            "1/1 [==============================] - 0s 83ms/step\n",
            "1/1 [==============================] - 0s 91ms/step\n",
            "1/1 [==============================] - 0s 83ms/step\n",
            "1/1 [==============================] - 0s 87ms/step\n",
            "1/1 [==============================] - 0s 89ms/step\n",
            "1/1 [==============================] - 0s 83ms/step\n",
            "1/1 [==============================] - 0s 94ms/step\n",
            "1/1 [==============================] - 0s 81ms/step\n",
            "1/1 [==============================] - 0s 88ms/step\n",
            "1/1 [==============================] - 0s 82ms/step\n",
            "1/1 [==============================] - 0s 84ms/step\n",
            "1/1 [==============================] - 0s 92ms/step\n",
            "1/1 [==============================] - 0s 88ms/step\n",
            "1/1 [==============================] - 0s 81ms/step\n",
            "1/1 [==============================] - 0s 91ms/step\n",
            "1/1 [==============================] - 0s 83ms/step\n",
            "1/1 [==============================] - 0s 151ms/step\n",
            "1/1 [==============================] - 0s 154ms/step\n",
            "1/1 [==============================] - 0s 94ms/step\n",
            "1/1 [==============================] - 0s 103ms/step\n",
            "1/1 [==============================] - 0s 88ms/step\n",
            "1/1 [==============================] - 0s 82ms/step\n",
            "1/1 [==============================] - 0s 88ms/step\n",
            "1/1 [==============================] - 0s 91ms/step\n",
            "1/1 [==============================] - 0s 81ms/step\n",
            "1/1 [==============================] - 0s 82ms/step\n",
            "1/1 [==============================] - 0s 91ms/step\n",
            "1/1 [==============================] - 0s 91ms/step\n",
            "1/1 [==============================] - 0s 81ms/step\n",
            "1/1 [==============================] - 0s 84ms/step\n",
            "1/1 [==============================] - 0s 91ms/step\n",
            "1/1 [==============================] - 0s 85ms/step\n",
            "1/1 [==============================] - 0s 82ms/step\n",
            "1/1 [==============================] - 0s 85ms/step\n",
            "1/1 [==============================] - 0s 82ms/step\n",
            "1/1 [==============================] - 0s 89ms/step\n",
            "1/1 [==============================] - 0s 88ms/step\n",
            "1/1 [==============================] - 0s 86ms/step\n",
            "1/1 [==============================] - 0s 83ms/step\n",
            "1/1 [==============================] - 0s 91ms/step\n",
            "1/1 [==============================] - 0s 85ms/step\n",
            "1/1 [==============================] - 0s 82ms/step\n",
            "1/1 [==============================] - 0s 86ms/step\n",
            "1/1 [==============================] - 0s 84ms/step\n",
            "1/1 [==============================] - 0s 83ms/step\n",
            "1/1 [==============================] - 0s 94ms/step\n",
            "1/1 [==============================] - 0s 82ms/step\n",
            "1/1 [==============================] - 0s 94ms/step\n",
            "1/1 [==============================] - 0s 80ms/step\n",
            "1/1 [==============================] - 0s 86ms/step\n",
            "1/1 [==============================] - 0s 85ms/step\n",
            "1/1 [==============================] - 0s 84ms/step\n",
            "1/1 [==============================] - 0s 90ms/step\n",
            "1/1 [==============================] - 0s 82ms/step\n",
            "1/1 [==============================] - 0s 84ms/step\n",
            "1/1 [==============================] - 0s 82ms/step\n",
            "1/1 [==============================] - 0s 97ms/step\n",
            "1/1 [==============================] - 0s 134ms/step\n",
            "1/1 [==============================] - 0s 150ms/step\n",
            "1/1 [==============================] - 0s 129ms/step\n",
            "1/1 [==============================] - 0s 124ms/step\n",
            "1/1 [==============================] - 0s 149ms/step\n",
            "1/1 [==============================] - 0s 146ms/step\n",
            "1/1 [==============================] - 0s 138ms/step\n",
            "1/1 [==============================] - 0s 135ms/step\n",
            "1/1 [==============================] - 0s 129ms/step\n",
            "1/1 [==============================] - 0s 135ms/step\n",
            "1/1 [==============================] - 0s 144ms/step\n",
            "1/1 [==============================] - 0s 144ms/step\n",
            "1/1 [==============================] - 0s 155ms/step\n",
            "1/1 [==============================] - 0s 145ms/step\n",
            "1/1 [==============================] - 0s 148ms/step\n",
            "1/1 [==============================] - 0s 154ms/step\n",
            "1/1 [==============================] - 0s 153ms/step\n",
            "1/1 [==============================] - 0s 152ms/step\n",
            "1/1 [==============================] - 0s 152ms/step\n",
            "1/1 [==============================] - 0s 142ms/step\n",
            "1/1 [==============================] - 0s 138ms/step\n",
            "1/1 [==============================] - 0s 169ms/step\n",
            "1/1 [==============================] - 0s 146ms/step\n",
            "1/1 [==============================] - 0s 146ms/step\n",
            "1/1 [==============================] - 0s 142ms/step\n",
            "1/1 [==============================] - 0s 139ms/step\n",
            "1/1 [==============================] - 0s 176ms/step\n",
            "1/1 [==============================] - 0s 91ms/step\n",
            "1/1 [==============================] - 0s 92ms/step\n",
            "1/1 [==============================] - 0s 88ms/step\n",
            "1/1 [==============================] - 0s 86ms/step\n",
            "1/1 [==============================] - 0s 82ms/step\n",
            "1/1 [==============================] - 0s 86ms/step\n",
            "1/1 [==============================] - 0s 84ms/step\n",
            "1/1 [==============================] - 0s 108ms/step\n",
            "1/1 [==============================] - 0s 82ms/step\n",
            "1/1 [==============================] - 0s 87ms/step\n",
            "1/1 [==============================] - 0s 85ms/step\n",
            "1/1 [==============================] - 0s 88ms/step\n",
            "1/1 [==============================] - 0s 83ms/step\n",
            "1/1 [==============================] - 0s 81ms/step\n",
            "1/1 [==============================] - 0s 87ms/step\n",
            "1/1 [==============================] - 0s 90ms/step\n",
            "1/1 [==============================] - 0s 88ms/step\n",
            "1/1 [==============================] - 0s 88ms/step\n",
            "1/1 [==============================] - 0s 84ms/step\n",
            "1/1 [==============================] - 0s 86ms/step\n",
            "1/1 [==============================] - 0s 93ms/step\n",
            "1/1 [==============================] - 0s 84ms/step\n",
            "1/1 [==============================] - 0s 83ms/step\n",
            "1/1 [==============================] - 0s 94ms/step\n",
            "1/1 [==============================] - 0s 98ms/step\n",
            "1/1 [==============================] - 0s 83ms/step\n",
            "1/1 [==============================] - 0s 87ms/step\n",
            "1/1 [==============================] - 0s 88ms/step\n",
            "1/1 [==============================] - 0s 83ms/step\n",
            "1/1 [==============================] - 0s 85ms/step\n",
            "1/1 [==============================] - 0s 85ms/step\n",
            "1/1 [==============================] - 0s 85ms/step\n",
            "1/1 [==============================] - 0s 90ms/step\n",
            "1/1 [==============================] - 0s 94ms/step\n",
            "1/1 [==============================] - 0s 87ms/step\n",
            "1/1 [==============================] - 0s 93ms/step\n",
            "1/1 [==============================] - 0s 83ms/step\n",
            "1/1 [==============================] - 0s 82ms/step\n",
            "1/1 [==============================] - 0s 85ms/step\n",
            "1/1 [==============================] - 0s 83ms/step\n",
            "1/1 [==============================] - 0s 84ms/step\n",
            "1/1 [==============================] - 0s 91ms/step\n",
            "1/1 [==============================] - 0s 91ms/step\n",
            "1/1 [==============================] - 0s 87ms/step\n",
            "1/1 [==============================] - 0s 81ms/step\n",
            "1/1 [==============================] - 0s 84ms/step\n",
            "1/1 [==============================] - 0s 82ms/step\n",
            "1/1 [==============================] - 0s 81ms/step\n",
            "1/1 [==============================] - 0s 86ms/step\n",
            "1/1 [==============================] - 0s 79ms/step\n",
            "1/1 [==============================] - 0s 95ms/step\n",
            "1/1 [==============================] - 0s 85ms/step\n",
            "1/1 [==============================] - 0s 95ms/step\n",
            "1/1 [==============================] - 0s 94ms/step\n",
            "1/1 [==============================] - 0s 97ms/step\n",
            "1/1 [==============================] - 0s 86ms/step\n",
            "1/1 [==============================] - 0s 87ms/step\n",
            "1/1 [==============================] - 0s 103ms/step\n",
            "1/1 [==============================] - 0s 100ms/step\n",
            "1/1 [==============================] - 0s 95ms/step\n",
            "1/1 [==============================] - 0s 91ms/step\n",
            "1/1 [==============================] - 0s 89ms/step\n",
            "1/1 [==============================] - 0s 88ms/step\n",
            "1/1 [==============================] - 0s 87ms/step\n",
            "1/1 [==============================] - 0s 87ms/step\n",
            "1/1 [==============================] - 0s 87ms/step\n",
            "1/1 [==============================] - 0s 92ms/step\n",
            "1/1 [==============================] - 0s 85ms/step\n",
            "1/1 [==============================] - 0s 89ms/step\n",
            "1/1 [==============================] - 0s 86ms/step\n",
            "1/1 [==============================] - 0s 82ms/step\n",
            "1/1 [==============================] - 0s 87ms/step\n",
            "1/1 [==============================] - 0s 88ms/step\n",
            "1/1 [==============================] - 0s 89ms/step\n",
            "1/1 [==============================] - 0s 82ms/step\n",
            "1/1 [==============================] - 0s 88ms/step\n",
            "1/1 [==============================] - 0s 89ms/step\n",
            "1/1 [==============================] - 0s 81ms/step\n",
            "1/1 [==============================] - 0s 88ms/step\n",
            "1/1 [==============================] - 0s 85ms/step\n",
            "1/1 [==============================] - 0s 81ms/step\n",
            "1/1 [==============================] - 0s 94ms/step\n",
            "1/1 [==============================] - 0s 97ms/step\n",
            "1/1 [==============================] - 0s 133ms/step\n",
            "1/1 [==============================] - 0s 128ms/step\n",
            "1/1 [==============================] - 0s 133ms/step\n",
            "1/1 [==============================] - 0s 148ms/step\n",
            "1/1 [==============================] - 0s 133ms/step\n",
            "1/1 [==============================] - 0s 144ms/step\n",
            "1/1 [==============================] - 0s 137ms/step\n",
            "1/1 [==============================] - 0s 122ms/step\n",
            "1/1 [==============================] - 0s 138ms/step\n",
            "1/1 [==============================] - 0s 139ms/step\n",
            "1/1 [==============================] - 0s 148ms/step\n",
            "1/1 [==============================] - 0s 144ms/step\n",
            "1/1 [==============================] - 0s 139ms/step\n",
            "1/1 [==============================] - 0s 151ms/step\n",
            "1/1 [==============================] - 0s 150ms/step\n",
            "1/1 [==============================] - 0s 151ms/step\n",
            "1/1 [==============================] - 0s 147ms/step\n",
            "1/1 [==============================] - 0s 137ms/step\n",
            "1/1 [==============================] - 0s 157ms/step\n",
            "1/1 [==============================] - 0s 144ms/step\n",
            "1/1 [==============================] - 0s 139ms/step\n",
            "1/1 [==============================] - 0s 156ms/step\n",
            "1/1 [==============================] - 0s 140ms/step\n",
            "1/1 [==============================] - 0s 155ms/step\n",
            "1/1 [==============================] - 0s 139ms/step\n",
            "1/1 [==============================] - 0s 150ms/step\n",
            "1/1 [==============================] - 0s 152ms/step\n",
            "1/1 [==============================] - 0s 115ms/step\n",
            "1/1 [==============================] - 0s 88ms/step\n",
            "1/1 [==============================] - 0s 87ms/step\n",
            "1/1 [==============================] - 0s 82ms/step\n",
            "1/1 [==============================] - 0s 88ms/step\n",
            "1/1 [==============================] - 0s 88ms/step\n",
            "1/1 [==============================] - 0s 81ms/step\n",
            "1/1 [==============================] - 0s 84ms/step\n",
            "1/1 [==============================] - 0s 85ms/step\n",
            "1/1 [==============================] - 0s 87ms/step\n",
            "1/1 [==============================] - 0s 87ms/step\n",
            "1/1 [==============================] - 0s 81ms/step\n",
            "1/1 [==============================] - 0s 86ms/step\n",
            "1/1 [==============================] - 0s 85ms/step\n",
            "1/1 [==============================] - 0s 90ms/step\n",
            "1/1 [==============================] - 0s 94ms/step\n",
            "1/1 [==============================] - 0s 97ms/step\n",
            "1/1 [==============================] - 0s 83ms/step\n",
            "1/1 [==============================] - 0s 83ms/step\n",
            "1/1 [==============================] - 0s 85ms/step\n",
            "1/1 [==============================] - 0s 88ms/step\n",
            "1/1 [==============================] - 0s 82ms/step\n",
            "1/1 [==============================] - 0s 81ms/step\n",
            "1/1 [==============================] - 0s 92ms/step\n",
            "1/1 [==============================] - 0s 82ms/step\n",
            "1/1 [==============================] - 0s 99ms/step\n",
            "1/1 [==============================] - 0s 82ms/step\n",
            "1/1 [==============================] - 0s 91ms/step\n",
            "1/1 [==============================] - 0s 84ms/step\n",
            "1/1 [==============================] - 0s 82ms/step\n",
            "1/1 [==============================] - 0s 82ms/step\n",
            "1/1 [==============================] - 0s 82ms/step\n",
            "1/1 [==============================] - 0s 89ms/step\n",
            "1/1 [==============================] - 0s 87ms/step\n",
            "1/1 [==============================] - 0s 92ms/step\n",
            "1/1 [==============================] - 0s 85ms/step\n",
            "1/1 [==============================] - 0s 82ms/step\n",
            "1/1 [==============================] - 0s 98ms/step\n",
            "1/1 [==============================] - 0s 84ms/step\n",
            "1/1 [==============================] - 0s 82ms/step\n",
            "1/1 [==============================] - 0s 97ms/step\n",
            "1/1 [==============================] - 0s 103ms/step\n",
            "1/1 [==============================] - 0s 110ms/step\n",
            "1/1 [==============================] - 0s 87ms/step\n",
            "1/1 [==============================] - 0s 88ms/step\n",
            "1/1 [==============================] - 0s 84ms/step\n",
            "1/1 [==============================] - 0s 86ms/step\n",
            "1/1 [==============================] - 0s 98ms/step\n",
            "1/1 [==============================] - 0s 86ms/step\n",
            "1/1 [==============================] - 0s 95ms/step\n",
            "1/1 [==============================] - 0s 92ms/step\n",
            "1/1 [==============================] - 0s 100ms/step\n",
            "1/1 [==============================] - 0s 102ms/step\n",
            "1/1 [==============================] - 0s 137ms/step\n",
            "1/1 [==============================] - 0s 116ms/step\n",
            "1/1 [==============================] - 0s 90ms/step\n",
            "1/1 [==============================] - 0s 100ms/step\n",
            "1/1 [==============================] - 0s 91ms/step\n",
            "1/1 [==============================] - 0s 83ms/step\n",
            "1/1 [==============================] - 0s 84ms/step\n",
            "1/1 [==============================] - 0s 82ms/step\n",
            "1/1 [==============================] - 0s 81ms/step\n",
            "1/1 [==============================] - 0s 81ms/step\n",
            "1/1 [==============================] - 0s 92ms/step\n",
            "1/1 [==============================] - 0s 86ms/step\n",
            "1/1 [==============================] - 0s 90ms/step\n",
            "1/1 [==============================] - 0s 82ms/step\n",
            "1/1 [==============================] - 0s 89ms/step\n",
            "1/1 [==============================] - 0s 91ms/step\n",
            "1/1 [==============================] - 0s 83ms/step\n",
            "1/1 [==============================] - 0s 86ms/step\n",
            "1/1 [==============================] - 0s 84ms/step\n",
            "1/1 [==============================] - 0s 83ms/step\n",
            "1/1 [==============================] - 0s 85ms/step\n",
            "1/1 [==============================] - 0s 96ms/step\n",
            "1/1 [==============================] - 0s 84ms/step\n",
            "1/1 [==============================] - 0s 83ms/step\n",
            "1/1 [==============================] - 0s 84ms/step\n",
            "1/1 [==============================] - 0s 83ms/step\n",
            "1/1 [==============================] - 0s 85ms/step\n",
            "1/1 [==============================] - 0s 90ms/step\n",
            "1/1 [==============================] - 0s 134ms/step\n",
            "1/1 [==============================] - 0s 137ms/step\n",
            "1/1 [==============================] - 0s 121ms/step\n",
            "1/1 [==============================] - 0s 160ms/step\n",
            "1/1 [==============================] - 0s 137ms/step\n",
            "1/1 [==============================] - 0s 127ms/step\n",
            "1/1 [==============================] - 0s 133ms/step\n",
            "1/1 [==============================] - 0s 130ms/step\n",
            "1/1 [==============================] - 0s 126ms/step\n",
            "1/1 [==============================] - 0s 133ms/step\n",
            "1/1 [==============================] - 0s 150ms/step\n",
            "1/1 [==============================] - 0s 147ms/step\n",
            "1/1 [==============================] - 0s 160ms/step\n",
            "1/1 [==============================] - 0s 151ms/step\n",
            "1/1 [==============================] - 0s 137ms/step\n",
            "1/1 [==============================] - 0s 144ms/step\n",
            "1/1 [==============================] - 0s 137ms/step\n",
            "1/1 [==============================] - 0s 139ms/step\n",
            "1/1 [==============================] - 0s 150ms/step\n",
            "1/1 [==============================] - 0s 139ms/step\n",
            "1/1 [==============================] - 0s 152ms/step\n",
            "1/1 [==============================] - 0s 137ms/step\n",
            "1/1 [==============================] - 0s 141ms/step\n",
            "1/1 [==============================] - 0s 148ms/step\n",
            "1/1 [==============================] - 0s 153ms/step\n",
            "1/1 [==============================] - 0s 150ms/step\n",
            "1/1 [==============================] - 0s 148ms/step\n",
            "1/1 [==============================] - 0s 134ms/step\n",
            "1/1 [==============================] - 0s 95ms/step\n",
            "1/1 [==============================] - 0s 85ms/step\n",
            "1/1 [==============================] - 0s 83ms/step\n",
            "1/1 [==============================] - 0s 88ms/step\n",
            "1/1 [==============================] - 0s 86ms/step\n",
            "1/1 [==============================] - 0s 86ms/step\n",
            "1/1 [==============================] - 0s 83ms/step\n",
            "1/1 [==============================] - 0s 83ms/step\n",
            "1/1 [==============================] - 0s 89ms/step\n",
            "1/1 [==============================] - 0s 93ms/step\n",
            "1/1 [==============================] - 0s 83ms/step\n",
            "1/1 [==============================] - 0s 82ms/step\n",
            "1/1 [==============================] - 0s 86ms/step\n",
            "1/1 [==============================] - 0s 84ms/step\n",
            "1/1 [==============================] - 0s 85ms/step\n",
            "1/1 [==============================] - 0s 83ms/step\n",
            "1/1 [==============================] - 0s 87ms/step\n",
            "1/1 [==============================] - 0s 92ms/step\n",
            "1/1 [==============================] - 0s 86ms/step\n",
            "1/1 [==============================] - 0s 89ms/step\n",
            "1/1 [==============================] - 0s 90ms/step\n",
            "1/1 [==============================] - 0s 88ms/step\n",
            "1/1 [==============================] - 0s 83ms/step\n",
            "1/1 [==============================] - 0s 83ms/step\n",
            "1/1 [==============================] - 0s 87ms/step\n",
            "1/1 [==============================] - 0s 83ms/step\n",
            "1/1 [==============================] - 0s 87ms/step\n",
            "1/1 [==============================] - 0s 84ms/step\n",
            "1/1 [==============================] - 0s 92ms/step\n",
            "1/1 [==============================] - 0s 95ms/step\n",
            "1/1 [==============================] - 0s 86ms/step\n",
            "1/1 [==============================] - 0s 82ms/step\n",
            "1/1 [==============================] - 0s 81ms/step\n",
            "1/1 [==============================] - 0s 83ms/step\n",
            "1/1 [==============================] - 0s 96ms/step\n",
            "1/1 [==============================] - 0s 84ms/step\n",
            "1/1 [==============================] - 0s 86ms/step\n",
            "1/1 [==============================] - 0s 85ms/step\n",
            "1/1 [==============================] - 0s 87ms/step\n",
            "1/1 [==============================] - 0s 89ms/step\n",
            "1/1 [==============================] - 0s 83ms/step\n",
            "1/1 [==============================] - 0s 84ms/step\n",
            "1/1 [==============================] - 0s 85ms/step\n",
            "1/1 [==============================] - 0s 90ms/step\n",
            "1/1 [==============================] - 0s 92ms/step\n",
            "1/1 [==============================] - 0s 85ms/step\n",
            "1/1 [==============================] - 0s 86ms/step\n",
            "1/1 [==============================] - 0s 92ms/step\n",
            "1/1 [==============================] - 0s 84ms/step\n",
            "1/1 [==============================] - 0s 83ms/step\n",
            "1/1 [==============================] - 0s 89ms/step\n",
            "1/1 [==============================] - 0s 95ms/step\n",
            "1/1 [==============================] - 0s 85ms/step\n",
            "1/1 [==============================] - 0s 84ms/step\n",
            "1/1 [==============================] - 0s 92ms/step\n",
            "1/1 [==============================] - 0s 84ms/step\n",
            "1/1 [==============================] - 0s 83ms/step\n",
            "1/1 [==============================] - 0s 88ms/step\n",
            "1/1 [==============================] - 0s 87ms/step\n",
            "1/1 [==============================] - 0s 93ms/step\n",
            "1/1 [==============================] - 0s 100ms/step\n",
            "1/1 [==============================] - 0s 80ms/step\n",
            "1/1 [==============================] - 0s 90ms/step\n",
            "1/1 [==============================] - 0s 90ms/step\n",
            "1/1 [==============================] - 0s 85ms/step\n",
            "1/1 [==============================] - 0s 89ms/step\n",
            "1/1 [==============================] - 0s 89ms/step\n",
            "1/1 [==============================] - 0s 82ms/step\n",
            "1/1 [==============================] - 0s 97ms/step\n",
            "1/1 [==============================] - 0s 90ms/step\n",
            "1/1 [==============================] - 0s 85ms/step\n",
            "1/1 [==============================] - 0s 89ms/step\n",
            "1/1 [==============================] - 0s 88ms/step\n",
            "1/1 [==============================] - 0s 83ms/step\n",
            "1/1 [==============================] - 0s 82ms/step\n",
            "1/1 [==============================] - 0s 92ms/step\n",
            "1/1 [==============================] - 0s 82ms/step\n",
            "1/1 [==============================] - 0s 89ms/step\n",
            "1/1 [==============================] - 0s 90ms/step\n",
            "1/1 [==============================] - 0s 86ms/step\n",
            "1/1 [==============================] - 0s 88ms/step\n",
            "1/1 [==============================] - 0s 82ms/step\n",
            "1/1 [==============================] - 0s 85ms/step\n",
            "1/1 [==============================] - 0s 139ms/step\n",
            "1/1 [==============================] - 0s 165ms/step\n",
            "1/1 [==============================] - 0s 141ms/step\n",
            "1/1 [==============================] - 0s 138ms/step\n",
            "1/1 [==============================] - 0s 125ms/step\n",
            "1/1 [==============================] - 0s 129ms/step\n",
            "1/1 [==============================] - 0s 136ms/step\n",
            "1/1 [==============================] - 0s 146ms/step\n",
            "1/1 [==============================] - 0s 129ms/step\n",
            "1/1 [==============================] - 0s 145ms/step\n",
            "1/1 [==============================] - 0s 138ms/step\n",
            "1/1 [==============================] - 0s 158ms/step\n",
            "1/1 [==============================] - 0s 156ms/step\n",
            "1/1 [==============================] - 0s 156ms/step\n",
            "1/1 [==============================] - 0s 151ms/step\n",
            "1/1 [==============================] - 0s 162ms/step\n",
            "1/1 [==============================] - 0s 149ms/step\n",
            "1/1 [==============================] - 0s 146ms/step\n",
            "1/1 [==============================] - 0s 144ms/step\n",
            "1/1 [==============================] - 0s 151ms/step\n",
            "1/1 [==============================] - 0s 148ms/step\n",
            "1/1 [==============================] - 0s 144ms/step\n",
            "1/1 [==============================] - 0s 138ms/step\n",
            "1/1 [==============================] - 0s 149ms/step\n",
            "1/1 [==============================] - 0s 141ms/step\n",
            "1/1 [==============================] - 0s 153ms/step\n",
            "1/1 [==============================] - 0s 138ms/step\n",
            "1/1 [==============================] - 0s 87ms/step\n",
            "1/1 [==============================] - 0s 82ms/step\n",
            "1/1 [==============================] - 0s 98ms/step\n",
            "1/1 [==============================] - 0s 89ms/step\n",
            "1/1 [==============================] - 0s 87ms/step\n",
            "1/1 [==============================] - 0s 82ms/step\n",
            "1/1 [==============================] - 0s 82ms/step\n",
            "1/1 [==============================] - 0s 89ms/step\n",
            "1/1 [==============================] - 0s 83ms/step\n",
            "1/1 [==============================] - 0s 82ms/step\n",
            "1/1 [==============================] - 0s 91ms/step\n",
            "1/1 [==============================] - 0s 85ms/step\n",
            "1/1 [==============================] - 0s 86ms/step\n",
            "1/1 [==============================] - 0s 87ms/step\n",
            "1/1 [==============================] - 0s 88ms/step\n",
            "1/1 [==============================] - 0s 85ms/step\n",
            "1/1 [==============================] - 0s 83ms/step\n",
            "1/1 [==============================] - 0s 82ms/step\n",
            "1/1 [==============================] - 0s 92ms/step\n",
            "1/1 [==============================] - 0s 106ms/step\n",
            "1/1 [==============================] - 0s 86ms/step\n",
            "1/1 [==============================] - 0s 82ms/step\n",
            "1/1 [==============================] - 0s 82ms/step\n",
            "1/1 [==============================] - 0s 83ms/step\n",
            "1/1 [==============================] - 0s 83ms/step\n",
            "1/1 [==============================] - 0s 88ms/step\n",
            "1/1 [==============================] - 0s 87ms/step\n",
            "1/1 [==============================] - 0s 99ms/step\n",
            "1/1 [==============================] - 0s 84ms/step\n",
            "1/1 [==============================] - 0s 87ms/step\n",
            "1/1 [==============================] - 0s 82ms/step\n",
            "1/1 [==============================] - 0s 91ms/step\n",
            "1/1 [==============================] - 0s 88ms/step\n",
            "1/1 [==============================] - 0s 82ms/step\n",
            "1/1 [==============================] - 0s 81ms/step\n",
            "1/1 [==============================] - 0s 91ms/step\n",
            "1/1 [==============================] - 0s 84ms/step\n",
            "1/1 [==============================] - 0s 81ms/step\n",
            "1/1 [==============================] - 0s 86ms/step\n",
            "1/1 [==============================] - 0s 85ms/step\n",
            "1/1 [==============================] - 0s 82ms/step\n",
            "1/1 [==============================] - 0s 86ms/step\n",
            "1/1 [==============================] - 0s 82ms/step\n",
            "1/1 [==============================] - 0s 87ms/step\n",
            "1/1 [==============================] - 0s 95ms/step\n",
            "1/1 [==============================] - 0s 85ms/step\n",
            "1/1 [==============================] - 0s 82ms/step\n",
            "1/1 [==============================] - 0s 82ms/step\n",
            "1/1 [==============================] - 0s 90ms/step\n",
            "1/1 [==============================] - 0s 89ms/step\n",
            "1/1 [==============================] - 0s 83ms/step\n",
            "1/1 [==============================] - 0s 82ms/step\n",
            "1/1 [==============================] - 0s 96ms/step\n",
            "1/1 [==============================] - 0s 89ms/step\n",
            "1/1 [==============================] - 0s 88ms/step\n",
            "1/1 [==============================] - 0s 81ms/step\n",
            "1/1 [==============================] - 0s 85ms/step\n",
            "1/1 [==============================] - 0s 87ms/step\n",
            "1/1 [==============================] - 0s 90ms/step\n",
            "1/1 [==============================] - 0s 82ms/step\n",
            "1/1 [==============================] - 0s 86ms/step\n",
            "1/1 [==============================] - 0s 96ms/step\n",
            "1/1 [==============================] - 0s 85ms/step\n",
            "1/1 [==============================] - 0s 85ms/step\n",
            "1/1 [==============================] - 0s 89ms/step\n",
            "1/1 [==============================] - 0s 82ms/step\n",
            "1/1 [==============================] - 0s 83ms/step\n",
            "1/1 [==============================] - 0s 87ms/step\n",
            "1/1 [==============================] - 0s 87ms/step\n",
            "1/1 [==============================] - 0s 84ms/step\n",
            "1/1 [==============================] - 0s 81ms/step\n",
            "1/1 [==============================] - 0s 85ms/step\n",
            "1/1 [==============================] - 0s 89ms/step\n",
            "1/1 [==============================] - 0s 85ms/step\n",
            "1/1 [==============================] - 0s 156ms/step\n",
            "1/1 [==============================] - 0s 142ms/step\n",
            "1/1 [==============================] - 0s 97ms/step\n",
            "1/1 [==============================] - 0s 96ms/step\n",
            "1/1 [==============================] - 0s 88ms/step\n",
            "1/1 [==============================] - 0s 87ms/step\n",
            "1/1 [==============================] - 0s 84ms/step\n",
            "1/1 [==============================] - 0s 117ms/step\n",
            "1/1 [==============================] - 0s 141ms/step\n",
            "1/1 [==============================] - 0s 130ms/step\n",
            "1/1 [==============================] - 0s 125ms/step\n",
            "1/1 [==============================] - 0s 127ms/step\n",
            "1/1 [==============================] - 0s 133ms/step\n",
            "1/1 [==============================] - 0s 141ms/step\n",
            "1/1 [==============================] - 0s 140ms/step\n",
            "1/1 [==============================] - 0s 135ms/step\n",
            "1/1 [==============================] - 0s 134ms/step\n",
            "1/1 [==============================] - 0s 129ms/step\n",
            "1/1 [==============================] - 0s 140ms/step\n",
            "1/1 [==============================] - 0s 150ms/step\n",
            "1/1 [==============================] - 0s 144ms/step\n",
            "1/1 [==============================] - 0s 166ms/step\n",
            "1/1 [==============================] - 0s 155ms/step\n",
            "1/1 [==============================] - 0s 155ms/step\n",
            "1/1 [==============================] - 0s 147ms/step\n",
            "1/1 [==============================] - 0s 144ms/step\n",
            "1/1 [==============================] - 0s 155ms/step\n",
            "1/1 [==============================] - 0s 141ms/step\n",
            "1/1 [==============================] - 0s 146ms/step\n",
            "1/1 [==============================] - 0s 141ms/step\n",
            "1/1 [==============================] - 0s 143ms/step\n",
            "1/1 [==============================] - 0s 157ms/step\n",
            "1/1 [==============================] - 0s 164ms/step\n",
            "1/1 [==============================] - 0s 165ms/step\n",
            "1/1 [==============================] - 0s 165ms/step\n",
            "1/1 [==============================] - 0s 121ms/step\n",
            "1/1 [==============================] - 0s 89ms/step\n",
            "1/1 [==============================] - 0s 85ms/step\n",
            "1/1 [==============================] - 0s 88ms/step\n",
            "1/1 [==============================] - 0s 82ms/step\n",
            "1/1 [==============================] - 0s 87ms/step\n",
            "1/1 [==============================] - 0s 85ms/step\n",
            "1/1 [==============================] - 0s 91ms/step\n",
            "1/1 [==============================] - 0s 86ms/step\n",
            "1/1 [==============================] - 0s 89ms/step\n",
            "1/1 [==============================] - 0s 97ms/step\n",
            "1/1 [==============================] - 0s 82ms/step\n",
            "1/1 [==============================] - 0s 92ms/step\n",
            "1/1 [==============================] - 0s 83ms/step\n",
            "1/1 [==============================] - 0s 92ms/step\n",
            "1/1 [==============================] - 0s 92ms/step\n",
            "1/1 [==============================] - 0s 97ms/step\n",
            "1/1 [==============================] - 0s 87ms/step\n",
            "1/1 [==============================] - 0s 98ms/step\n",
            "1/1 [==============================] - 0s 87ms/step\n",
            "1/1 [==============================] - 0s 85ms/step\n",
            "1/1 [==============================] - 0s 86ms/step\n",
            "1/1 [==============================] - 0s 86ms/step\n",
            "1/1 [==============================] - 0s 87ms/step\n",
            "1/1 [==============================] - 0s 89ms/step\n",
            "1/1 [==============================] - 0s 100ms/step\n",
            "1/1 [==============================] - 0s 96ms/step\n",
            "1/1 [==============================] - 0s 83ms/step\n",
            "1/1 [==============================] - 0s 83ms/step\n",
            "1/1 [==============================] - 0s 95ms/step\n",
            "1/1 [==============================] - 0s 83ms/step\n",
            "1/1 [==============================] - 0s 85ms/step\n",
            "1/1 [==============================] - 0s 90ms/step\n",
            "1/1 [==============================] - 0s 82ms/step\n",
            "1/1 [==============================] - 0s 91ms/step\n",
            "1/1 [==============================] - 0s 91ms/step\n",
            "1/1 [==============================] - 0s 88ms/step\n",
            "1/1 [==============================] - 0s 85ms/step\n",
            "1/1 [==============================] - 0s 86ms/step\n",
            "1/1 [==============================] - 0s 86ms/step\n",
            "1/1 [==============================] - 0s 93ms/step\n",
            "1/1 [==============================] - 0s 88ms/step\n",
            "1/1 [==============================] - 0s 83ms/step\n",
            "1/1 [==============================] - 0s 100ms/step\n",
            "1/1 [==============================] - 0s 85ms/step\n",
            "1/1 [==============================] - 0s 95ms/step\n",
            "1/1 [==============================] - 0s 84ms/step\n",
            "1/1 [==============================] - 0s 92ms/step\n",
            "1/1 [==============================] - 0s 88ms/step\n",
            "1/1 [==============================] - 0s 87ms/step\n",
            "1/1 [==============================] - 0s 92ms/step\n",
            "1/1 [==============================] - 0s 106ms/step\n",
            "1/1 [==============================] - 0s 88ms/step\n",
            "1/1 [==============================] - 0s 90ms/step\n",
            "1/1 [==============================] - 0s 86ms/step\n",
            "1/1 [==============================] - 0s 85ms/step\n",
            "1/1 [==============================] - 0s 91ms/step\n",
            "1/1 [==============================] - 0s 85ms/step\n",
            "1/1 [==============================] - 0s 87ms/step\n",
            "1/1 [==============================] - 0s 90ms/step\n",
            "1/1 [==============================] - 0s 88ms/step\n",
            "1/1 [==============================] - 0s 89ms/step\n",
            "1/1 [==============================] - 0s 82ms/step\n",
            "1/1 [==============================] - 0s 92ms/step\n",
            "1/1 [==============================] - 0s 89ms/step\n",
            "1/1 [==============================] - 0s 81ms/step\n",
            "1/1 [==============================] - 0s 84ms/step\n",
            "1/1 [==============================] - 0s 83ms/step\n",
            "1/1 [==============================] - 0s 97ms/step\n",
            "1/1 [==============================] - 0s 82ms/step\n",
            "1/1 [==============================] - 0s 81ms/step\n",
            "1/1 [==============================] - 0s 83ms/step\n",
            "1/1 [==============================] - 0s 91ms/step\n",
            "1/1 [==============================] - 0s 84ms/step\n",
            "1/1 [==============================] - 0s 87ms/step\n",
            "1/1 [==============================] - 0s 82ms/step\n",
            "1/1 [==============================] - 0s 101ms/step\n",
            "1/1 [==============================] - 0s 81ms/step\n",
            "1/1 [==============================] - 0s 84ms/step\n",
            "1/1 [==============================] - 0s 99ms/step\n",
            "1/1 [==============================] - 0s 90ms/step\n",
            "1/1 [==============================] - 0s 108ms/step\n",
            "1/1 [==============================] - 0s 148ms/step\n",
            "1/1 [==============================] - 0s 149ms/step\n",
            "1/1 [==============================] - 0s 136ms/step\n",
            "1/1 [==============================] - 0s 134ms/step\n",
            "1/1 [==============================] - 0s 142ms/step\n",
            "1/1 [==============================] - 0s 138ms/step\n",
            "1/1 [==============================] - 0s 137ms/step\n",
            "1/1 [==============================] - 0s 166ms/step\n",
            "1/1 [==============================] - 0s 136ms/step\n",
            "1/1 [==============================] - 0s 153ms/step\n",
            "1/1 [==============================] - 0s 137ms/step\n",
            "1/1 [==============================] - 0s 151ms/step\n",
            "1/1 [==============================] - 0s 156ms/step\n",
            "1/1 [==============================] - 0s 161ms/step\n",
            "1/1 [==============================] - 0s 161ms/step\n",
            "1/1 [==============================] - 0s 142ms/step\n",
            "1/1 [==============================] - 0s 137ms/step\n",
            "1/1 [==============================] - 0s 151ms/step\n",
            "1/1 [==============================] - 0s 149ms/step\n",
            "1/1 [==============================] - 0s 145ms/step\n",
            "1/1 [==============================] - 0s 143ms/step\n",
            "1/1 [==============================] - 0s 152ms/step\n",
            "1/1 [==============================] - 0s 143ms/step\n",
            "1/1 [==============================] - 0s 157ms/step\n",
            "1/1 [==============================] - 0s 150ms/step\n",
            "1/1 [==============================] - 0s 97ms/step\n",
            "1/1 [==============================] - 0s 87ms/step\n",
            "1/1 [==============================] - 0s 97ms/step\n",
            "1/1 [==============================] - 0s 91ms/step\n",
            "1/1 [==============================] - 0s 90ms/step\n",
            "1/1 [==============================] - 0s 88ms/step\n",
            "1/1 [==============================] - 0s 95ms/step\n",
            "1/1 [==============================] - 0s 87ms/step\n",
            "1/1 [==============================] - 0s 105ms/step\n",
            "1/1 [==============================] - 0s 95ms/step\n",
            "1/1 [==============================] - 0s 95ms/step\n",
            "1/1 [==============================] - 0s 91ms/step\n",
            "1/1 [==============================] - 0s 94ms/step\n",
            "1/1 [==============================] - 0s 85ms/step\n",
            "1/1 [==============================] - 0s 86ms/step\n",
            "1/1 [==============================] - 0s 94ms/step\n",
            "1/1 [==============================] - 0s 85ms/step\n",
            "1/1 [==============================] - 0s 98ms/step\n",
            "1/1 [==============================] - 0s 90ms/step\n",
            "1/1 [==============================] - 0s 96ms/step\n",
            "1/1 [==============================] - 0s 95ms/step\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/func_graph.py\", line 1055, in func_graph_from_py_func\n",
            "    func_outputs = python_func(*func_args, **func_kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\", line 597, in wrapped_fn\n",
            "    out = weak_wrapped_fn().__wrapped__(*args, **kwds)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/data/ops/structured_function.py\", line 238, in wrapped_fn\n",
            "    ret = wrapper_helper(*args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/data/ops/structured_function.py\", line 168, in wrapper_helper\n",
            "    ret = autograph.tf_convert(self._func, ag_ctx)(*nested_args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/autograph/impl/api.py\", line 690, in wrapper\n",
            "    return converted_call(f, args, kwargs, options=options)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/autograph/impl/api.py\", line 377, in converted_call\n",
            "    return _call_unconverted(f, args, kwargs, options)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/autograph/impl/api.py\", line 459, in _call_unconverted\n",
            "    return f(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/data_adapter.py\", line 342, in slice_batch_indices\n",
            "    tf.slice(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\", line 150, in error_handler\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/dispatch.py\", line 1176, in op_dispatch_handler\n",
            "    return dispatch_target(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/ops/array_ops.py\", line 1185, in slice\n",
            "    return gen_array_ops._slice(input_, begin, size, name=name)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/ops/gen_array_ops.py\", line 9614, in _slice\n",
            "    _, _, _op, _outputs = _op_def_library._apply_op_helper(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/op_def_library.py\", line 782, in _apply_op_helper\n",
            "    _ExtractAttrProto(op_type_name, op_def, attrs, attr_protos)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/op_def_library.py\", line 328, in _ExtractAttrProto\n",
            "    attr_value = value_to_attr_value(value, attr_def.type, key)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/op_def_library.py\", line 851, in value_to_attr_value\n",
            "    attr_value.type = _MakeType(value, arg_name)\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"apache_beam/runners/common.py\", line 1423, in apache_beam.runners.common.DoFnRunner.process\n",
            "  File \"apache_beam/runners/common.py\", line 625, in apache_beam.runners.common.SimpleInvoker.invoke_process\n",
            "  File \"apache_beam/runners/common.py\", line 1597, in apache_beam.runners.common._OutputHandler.handle_process_outputs\n",
            "  File \"/content/batch/predict.py\", line 73, in process\n",
            "    \"text\": element,\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 2521, in predict\n",
            "    data_handler = data_adapter.get_data_handler(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/data_adapter.py\", line 1678, in get_data_handler\n",
            "    return DataHandler(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/data_adapter.py\", line 1285, in __init__\n",
            "    self._adapter = adapter_cls(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/data_adapter.py\", line 353, in __init__\n",
            "    indices_dataset = indices_dataset.flat_map(slice_batch_indices)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/data/ops/dataset_ops.py\", line 2323, in flat_map\n",
            "    return flat_map_op._flat_map(self, map_func, name=name)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/data/ops/flat_map_op.py\", line 24, in _flat_map\n",
            "    return _FlatMapDataset(input_dataset, map_func, name)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/data/ops/flat_map_op.py\", line 33, in __init__\n",
            "    self._map_func = structured_function.StructuredFunctionWrapper(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/data/ops/structured_function.py\", line 272, in __init__\n",
            "    self._function = fn_factory()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\", line 1189, in get_concrete_function\n",
            "    concrete = self._get_concrete_function_garbage_collected(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\", line 1169, in _get_concrete_function_garbage_collected\n",
            "    self._initialize(args, kwargs, add_initializers_to=initializers)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\", line 694, in _initialize\n",
            "    self._variable_creation_fn    # pylint: disable=protected-access\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py\", line 176, in _get_concrete_function_internal_garbage_collected\n",
            "    concrete_function, _ = self._maybe_define_concrete_function(args, kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py\", line 171, in _maybe_define_concrete_function\n",
            "    return self._maybe_define_function(args, kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py\", line 398, in _maybe_define_function\n",
            "    concrete_function = self._create_concrete_function(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py\", line 305, in _create_concrete_function\n",
            "    func_graph_module.func_graph_from_py_func(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/func_graph.py\", line 987, in func_graph_from_py_func\n",
            "    with func_graph.as_default(), deps_control_manager as deps_ctx:\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/auto_control_deps.py\", line 343, in __exit__\n",
            "    def __exit__(self, unused_type, unused_value, unused_traceback):\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/batch/predict.py\", line 166, in <module>\n",
            "    run(args.model_dir, source, sink, beam_options)\n",
            "  File \"/content/batch/predict.py\", line 99, in run\n",
            "    with beam.Pipeline(options=beam_options) as p:\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/apache_beam/pipeline.py\", line 600, in __exit__\n",
            "    self.result = self.run()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/apache_beam/pipeline.py\", line 577, in run\n",
            "    return self.runner.run_pipeline(self, self._options)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/apache_beam/runners/direct/direct_runner.py\", line 128, in run_pipeline\n",
            "    return runner.run_pipeline(pipeline, options)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/apache_beam/runners/portability/fn_api_runner/fn_runner.py\", line 202, in run_pipeline\n",
            "    self._latest_run_result = self.run_via_runner_api(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/apache_beam/runners/portability/fn_api_runner/fn_runner.py\", line 224, in run_via_runner_api\n",
            "    return self.run_stages(stage_context, stages)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/apache_beam/runners/portability/fn_api_runner/fn_runner.py\", line 455, in run_stages\n",
            "    bundle_results = self._execute_bundle(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/apache_beam/runners/portability/fn_api_runner/fn_runner.py\", line 783, in _execute_bundle\n",
            "    self._run_bundle(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/apache_beam/runners/portability/fn_api_runner/fn_runner.py\", line 1020, in _run_bundle\n",
            "    result, splits = bundle_manager.process_bundle(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/apache_beam/runners/portability/fn_api_runner/fn_runner.py\", line 1356, in process_bundle\n",
            "    result_future = self._worker_handler.control_conn.push(process_bundle_req)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/apache_beam/runners/portability/fn_api_runner/worker_handlers.py\", line 379, in push\n",
            "    response = self.worker.do_instruction(request)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/apache_beam/runners/worker/sdk_worker.py\", line 625, in do_instruction\n",
            "    return getattr(self, request_type)(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/apache_beam/runners/worker/sdk_worker.py\", line 663, in process_bundle\n",
            "    bundle_processor.process_bundle(instruction_id))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/apache_beam/runners/worker/bundle_processor.py\", line 1051, in process_bundle\n",
            "    input_op_by_transform_id[element.transform_id].process_encoded(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/apache_beam/runners/worker/bundle_processor.py\", line 232, in process_encoded\n",
            "    self.output(decoded_value)\n",
            "  File \"apache_beam/runners/worker/operations.py\", line 568, in apache_beam.runners.worker.operations.Operation.output\n",
            "  File \"apache_beam/runners/worker/operations.py\", line 570, in apache_beam.runners.worker.operations.Operation.output\n",
            "  File \"apache_beam/runners/worker/operations.py\", line 261, in apache_beam.runners.worker.operations.SingletonElementConsumerSet.receive\n",
            "  File \"apache_beam/runners/worker/operations.py\", line 264, in apache_beam.runners.worker.operations.SingletonElementConsumerSet.receive\n",
            "  File \"apache_beam/runners/worker/operations.py\", line 1065, in apache_beam.runners.worker.operations.SdfProcessSizedElements.process\n",
            "  File \"apache_beam/runners/worker/operations.py\", line 1074, in apache_beam.runners.worker.operations.SdfProcessSizedElements.process\n",
            "  File \"apache_beam/runners/common.py\", line 1454, in apache_beam.runners.common.DoFnRunner.process_with_sized_restriction\n",
            "  File \"apache_beam/runners/common.py\", line 819, in apache_beam.runners.common.PerWindowInvoker.invoke_process\n",
            "  File \"apache_beam/runners/common.py\", line 983, in apache_beam.runners.common.PerWindowInvoker._invoke_process_per_window\n",
            "  File \"apache_beam/runners/common.py\", line 1607, in apache_beam.runners.common._OutputHandler.handle_process_outputs\n",
            "  File \"apache_beam/runners/common.py\", line 1720, in apache_beam.runners.common._OutputHandler._write_value_to_tag\n",
            "  File \"apache_beam/runners/worker/operations.py\", line 264, in apache_beam.runners.worker.operations.SingletonElementConsumerSet.receive\n",
            "  File \"apache_beam/runners/worker/operations.py\", line 951, in apache_beam.runners.worker.operations.DoOperation.process\n",
            "  File \"apache_beam/runners/worker/operations.py\", line 952, in apache_beam.runners.worker.operations.DoOperation.process\n",
            "  File \"apache_beam/runners/common.py\", line 1425, in apache_beam.runners.common.DoFnRunner.process\n",
            "  File \"apache_beam/runners/common.py\", line 1533, in apache_beam.runners.common.DoFnRunner._reraise_augmented\n",
            "  File \"apache_beam/runners/common.py\", line 1423, in apache_beam.runners.common.DoFnRunner.process\n",
            "  File \"apache_beam/runners/common.py\", line 625, in apache_beam.runners.common.SimpleInvoker.invoke_process\n",
            "  File \"apache_beam/runners/common.py\", line 1597, in apache_beam.runners.common._OutputHandler.handle_process_outputs\n",
            "  File \"/content/batch/predict.py\", line 73, in process\n",
            "    \"text\": element,\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 2521, in predict\n",
            "    data_handler = data_adapter.get_data_handler(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/data_adapter.py\", line 1678, in get_data_handler\n",
            "    return DataHandler(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/data_adapter.py\", line 1285, in __init__\n",
            "    self._adapter = adapter_cls(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/data_adapter.py\", line 353, in __init__\n",
            "    indices_dataset = indices_dataset.flat_map(slice_batch_indices)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/data/ops/dataset_ops.py\", line 2323, in flat_map\n",
            "    return flat_map_op._flat_map(self, map_func, name=name)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/data/ops/flat_map_op.py\", line 24, in _flat_map\n",
            "    return _FlatMapDataset(input_dataset, map_func, name)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/data/ops/flat_map_op.py\", line 33, in __init__\n",
            "    self._map_func = structured_function.StructuredFunctionWrapper(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/data/ops/structured_function.py\", line 272, in __init__\n",
            "    self._function = fn_factory()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\", line 1189, in get_concrete_function\n",
            "    concrete = self._get_concrete_function_garbage_collected(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\", line 1169, in _get_concrete_function_garbage_collected\n",
            "    self._initialize(args, kwargs, add_initializers_to=initializers)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\", line 694, in _initialize\n",
            "    self._variable_creation_fn    # pylint: disable=protected-access\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py\", line 176, in _get_concrete_function_internal_garbage_collected\n",
            "    concrete_function, _ = self._maybe_define_concrete_function(args, kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py\", line 171, in _maybe_define_concrete_function\n",
            "    return self._maybe_define_function(args, kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py\", line 398, in _maybe_define_function\n",
            "    concrete_function = self._create_concrete_function(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py\", line 305, in _create_concrete_function\n",
            "    func_graph_module.func_graph_from_py_func(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/func_graph.py\", line 987, in func_graph_from_py_func\n",
            "    with func_graph.as_default(), deps_control_manager as deps_ctx:\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/auto_control_deps.py\", line 343, in __exit__\n",
            "    def __exit__(self, unused_type, unused_value, unused_traceback):\n",
            "RuntimeError: KeyboardInterrupt [while running 'Predict']\n"
          ]
        }
      ],
      "source": [
        "! python3 predict.py \\\n",
        "  --work-dir $WORK_DIR \\\n",
        "  --model-dir $WORK_DIR/data/model \\\n",
        "  batch \\\n",
        "  --inputs-dir $WORK_DIR/transformed_data/test/part* \\\n",
        "  --outputs-dir $WORK_DIR/predictions/$TIMESTAMP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Um68jx_F8mjF"
      },
      "source": [
        "## Extra. Comprobaciones en la nube\n",
        "\n",
        "En esta última parte se validará el funcionamiento del código en un proyecto de GCP sobre DataFlow y AI Platform"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "crPjZuOVuM-f"
      },
      "source": [
        "Me copio los datos necesarios a Google Cloud:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xN16t4Myukcm",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ybu7kzf7uR1x",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "! gsutil cp -r model gs://$BUCKET_NAME"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6jSwelYiu5p8",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "! gsutil cp -r transformed_data gs://$BUCKET_NAME"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_JHC9dCT83FH"
      },
      "source": [
        "Establecemos el bucket y region de GCP sobre el que trabajaremos:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cgUpX__KQvt-",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "GCP_WORK_DIR = 'gs://' + BUCKET_NAME\n",
        "GCP_REGION = \"europe-west1\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_A66WAfm9DmU"
      },
      "source": [
        "### Validación preprocess train en Dataflow\n",
        "\n",
        "Con el comando mostrado a continuación se valida la correcta generación de los datos de entrenamiento y validación en GCP con el servicio DataFlow.\n",
        "\n",
        "**Nota.** Recuerda detener la celda una vez el trabajo te aparezca ejecutando en DataFlow."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EJYVuWyJP1Ya",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "! python3 preprocess.py \\\n",
        "  --project $PROJECT_ID \\\n",
        "  --region $GCP_REGION \\\n",
        "  --runner DataflowRunner \\\n",
        "  --temp_location $GCP_WORK_DIR/beam-temp \\\n",
        "  --setup_file ./setup.py \\\n",
        "  --work-dir $GCP_WORK_DIR \\\n",
        "  --input $GCP_WORK_DIR/data.json \\\n",
        "  --output $GCP_WORK_DIR/transformed_data \\\n",
        "  --mode train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8czKRMb99ZRX"
      },
      "source": [
        "### Validación preprocess test en Dataflow\n",
        "\n",
        "Con el comando mostrado a continuación se valida la correcta generación de los datos de test en GCP con el servicio DataFlow.\n",
        "\n",
        "**Nota.** Recuerda detener la celda una vez el trabajo te aparezca ejecutando en DataFlow."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "45mfSRGdQLBO",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "! python3 preprocess.py \\\n",
        "  --project $PROJECT_ID \\\n",
        "  --region $GCP_REGION \\\n",
        "  --runner DataflowRunner \\\n",
        "  --temp_location $GCP_WORK_DIR/beam-temp \\\n",
        "  --setup_file ./setup.py \\\n",
        "  --work-dir $GCP_WORK_DIR \\\n",
        "  --input $GCP_WORK_DIR/data.json \\\n",
        "  --output $GCP_WORK_DIR/transformed_data \\\n",
        "  --mode test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sTVj6bqII_3m"
      },
      "source": [
        "Omitimos el entrenamiento en la nube para no incurrir en costes innecesarios."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wu5SOWsy9886"
      },
      "source": [
        "### Validación predict en Dataflow\n",
        "\n",
        "Con el comando mostrado a continuación se valida la predicción correcta de los datos de test usando los modelos generados en el comando anterior."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OXoHCgMg-LUD"
      },
      "source": [
        "Generamos un timestamp para el almacenamiento de las inferencias en Google Cloud Storage."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fmcetQtnQjVo",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "from datetime import datetime\n",
        "\n",
        "# current date and time\n",
        "TIMESTAMP = datetime.now().strftime('%Y-%m-%d_%H-%M-%S')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qRsZaneP0waK"
      },
      "source": [
        "**Nota.** Recuerda detener la celda una vez el trabajo te aparezca ejecutando en DataFlow."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zT_NxsKDQlxD",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# For using sample models: --model-dir gs://$BUCKET_NAME/models/\n",
        "! python3 predict.py \\\n",
        "  --work-dir $GCP_WORK_DIR \\\n",
        "  --model-dir $GCP_WORK_DIR/model/ \\\n",
        "  batch \\\n",
        "  --project $PROJECT_ID \\\n",
        "  --region $GCP_REGION \\\n",
        "  --runner DataflowRunner \\\n",
        "  --temp_location $GCP_WORK_DIR/beam-temp \\\n",
        "  --setup_file ./setup.py \\\n",
        "  --inputs-dir $GCP_WORK_DIR/transformed_data/test/part* \\\n",
        "  --outputs-dir $GCP_WORK_DIR/predictions/$TIMESTAMP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "co3YBKjscCVI"
      },
      "source": [
        "# Mensaje final\n",
        "\n",
        "¡Muchas gracias por participar en este curso, espero que tanto las sesiones teóricas como la práctica te hayan resultado útiles. A lo largo de esta semmana iréis recibiendo feedback personalizado sobre vuestras prácticas.\n",
        "\n",
        "¡Muchas gracias y ánimo con el proyecto final!"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}